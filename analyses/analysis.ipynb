{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/paularescala/Documents/Professional/Masters-Thesis-2023/debate-gpt\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debate_gpt.results_analysis.metrics import get_bootstrap\n",
    "from debate_gpt.results_analysis.helpers import get_train_test, get_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = pd.read_json(\"data/tidy/llm_outputs/q1.json\")\n",
    "q2 = pd.read_json(\"data/tidy/llm_outputs/q2.json\")\n",
    "q3 = pd.read_json(\"data/tidy/llm_outputs/q3.json\")\n",
    "binary = pd.read_json(\"data/tidy/llm_outputs/q2-binary.json\")\n",
    "issues = pd.read_json(\"data/tidy/llm_outputs/q2-issues.json\")\n",
    "\n",
    "with open(\"data/tidy/datasets/datasets.json\") as f:\n",
    "    dataset_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = list(q1.model.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for df, question in zip([q1, q2, q3], [\"q1\", \"q2\", \"q3\"]):\n",
    "    df[\"correct_form\"] = df.processed_gpt_response == df.gpt_response\n",
    "    df[\"answer_extracted\"] = df.processed_gpt_response.isin([\"Pro\", \"Con\", \"Tie\"])\n",
    "    df = df.groupby(\"model\")[[\"correct_form\", \"answer_extracted\"]].mean().reset_index()\n",
    "    df[\"question\"] = question\n",
    "    df[\"correct_form\"] = df.correct_form * 100\n",
    "    df[\"answer_extracted\"] = df.answer_extracted * 100\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_extraction_df = pd.concat(dfs)[\n",
    "    [\"question\", \"model\", \"correct_form\", \"answer_extracted\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tidy/latex_tables/answer_extraction.txt\", \"w\") as f:\n",
    "    f.write(\n",
    "        answer_extraction_df.to_latex(index=False, float_format=\"%.2f\", position=\"h\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "datasets = []\n",
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "\n",
    "for df, question in zip([q1, q2, q3], [\"q1\", \"q2\", \"q3\"]):\n",
    "    for dataset in [\"Trimmed\", \"Issues\"]:\n",
    "        for model in model_list:\n",
    "            model_df = df[df.model == model]\n",
    "            temp_df = model_df[model_df.debate_id.isin(dataset_dict[dataset])]\n",
    "            accuracy, _, _, ci = get_bootstrap(temp_df)\n",
    "\n",
    "            questions.append(question)\n",
    "            models.append(model)\n",
    "            datasets.append(dataset)\n",
    "            accuracies.append(accuracy)\n",
    "            confidence_intervals.append(ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Question\": questions,\n",
    "        \"Model\": models,\n",
    "        \"Dataset\": datasets,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "primary_table[\"Model\"] = (\n",
    "    primary_table.Model.str.capitalize()\n",
    "    .str.replace(\"Gpt-\", \"GPT-\")\n",
    "    .str.replace(\"Mturk\", \"MTurk\")\n",
    ")\n",
    "primary_table[\"Model\"] = pd.Categorical(\n",
    "    primary_table[\"Model\"], [\"Llama\", \"Mistral\", \"GPT-3.5\", \"GPT-4\", \"MTurk\"]\n",
    ")\n",
    "primary_table[\"Dataset\"] = pd.Categorical(\n",
    "    primary_table[\"Dataset\"], [\"Trimmed\", \"Short\", \"Issues\"]\n",
    ")\n",
    "primary_table = primary_table.sort_values([\"Question\", \"Dataset\", \"Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tidy/latex_tables/primary_results.txt\", \"w\") as f:\n",
    "    f.write(primary_table.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "models = []\n",
    "types = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "\n",
    "for dataset in [\"Trimmed\", \"Issues\"]:\n",
    "    for model in model_list:\n",
    "        model_df = binary[binary.model == model]\n",
    "        temp_df = model_df[model_df.debate_id.isin(dataset_dict[dataset])]\n",
    "        accuracy, _, _, ci = get_bootstrap(temp_df)\n",
    "\n",
    "        models.append(model)\n",
    "        datasets.append(dataset)\n",
    "        accuracies.append(accuracy)\n",
    "        confidence_intervals.append(ci)\n",
    "        types.append(\"Binary\")\n",
    "\n",
    "        model_df = q2[q2.model == model]\n",
    "        temp_df = model_df[model_df.debate_id.isin(list(temp_df.debate_id.unique()))]\n",
    "        accuracy, _, _, ci = get_bootstrap(temp_df)\n",
    "        models.append(model)\n",
    "        datasets.append(dataset)\n",
    "        accuracies.append(accuracy)\n",
    "        confidence_intervals.append(ci)\n",
    "        types.append(\"3-class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Dataset\": datasets,\n",
    "        \"Classes\": types,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "binary_table[\"Model\"] = (\n",
    "    binary_table.Model.str.capitalize()\n",
    "    .str.replace(\"Gpt-\", \"GPT-\")\n",
    "    .str.replace(\"Mturk\", \"MTurk\")\n",
    ")\n",
    "binary_table[\"Model\"] = pd.Categorical(\n",
    "    binary_table[\"Model\"], [\"Llama\", \"Mistral\", \"GPT-3.5\", \"GPT-4\", \"MTurk\"]\n",
    ")\n",
    "binary_table[\"Dataset\"] = pd.Categorical(\n",
    "    binary_table[\"Dataset\"], [\"Trimmed\", \"Short\", \"Issues\"]\n",
    ")\n",
    "binary_table = binary_table.sort_values([\"Dataset\", \"Classes\", \"Model\"])\n",
    "binary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tidy/latex_tables/binary_results.txt\", \"w\") as f:\n",
    "    f.write(binary_table.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues[\"big_issues\"] = issues.apply(\n",
    "    lambda x: False if x.model == \"MTurk\" else x.big_issues, axis=1\n",
    ")\n",
    "issues[\"reasoning\"] = issues.apply(\n",
    "    lambda x: True if x.model == \"MTurk\" else x.reasoning, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "big_issues = []\n",
    "reasoning = []\n",
    "\n",
    "for model in model_list:\n",
    "    for bi in [True, False]:\n",
    "        for r in [True, False]:\n",
    "            model_df = issues[issues.model == model]\n",
    "            temp_df = model_df[(model_df.reasoning == r) & (model_df.big_issues == bi)]\n",
    "            accuracy, _, _, ci = get_bootstrap(temp_df)\n",
    "\n",
    "            models.append(model)\n",
    "            accuracies.append(accuracy)\n",
    "            confidence_intervals.append(ci)\n",
    "            big_issues.append(bi)\n",
    "            reasoning.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Big Issues\": big_issues,\n",
    "        \"Reasoning\": reasoning,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "issues_table[\"Model\"] = (\n",
    "    issues_table.Model.str.capitalize()\n",
    "    .str.replace(\"Gpt-\", \"GPT-\")\n",
    "    .str.replace(\"Mturk\", \"MTurk\")\n",
    ")\n",
    "issues_table[\"Model\"] = pd.Categorical(\n",
    "    issues_table[\"Model\"], [\"Llama\", \"Mistral\", \"GPT-3.5\", \"GPT-4\", \"MTurk\"]\n",
    ")\n",
    "issues_table = issues_table.sort_values([\"Model\", \"Big Issues\", \"Reasoning\"])\n",
    "issues_table = issues_table.dropna(subset=[\"Accuracy (\\%)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abortion_df = pd.read_json(\"data/tidy/regression_files/abortion.json\")\n",
    "gay_marriage_df = pd.read_json(\"data/tidy/regression_files/gay_marriage.json\")\n",
    "capital_punishment_df = pd.read_json(\n",
    "    \"data/tidy/regression_files/capital_punishment.json\"\n",
    ")\n",
    "issues_dfs = [abortion_df, gay_marriage_df, capital_punishment_df]\n",
    "\n",
    "with open(\"config/task_configs.json\") as f:\n",
    "    task_config = json.load(f)\n",
    "\n",
    "demographic_features = task_config[\"demographic_columns\"]\n",
    "demographic_features.remove(\"birthday\")\n",
    "big_issues_features = task_config[\"big_issue_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_dataframes = []\n",
    "dummies_dataframes_BI = []\n",
    "features = []\n",
    "features_BI = []\n",
    "\n",
    "for df in issues_dfs:\n",
    "    df_dummies = pd.get_dummies(\n",
    "        df[[\"debate_id\", \"agreed_before\"] + demographic_features]\n",
    "    )\n",
    "    df_dummies_BI = pd.get_dummies(\n",
    "        df[[\"debate_id\", \"agreed_before\"] + demographic_features + big_issues_features]\n",
    "    )\n",
    "    dummies_dataframes.append(df_dummies)\n",
    "    dummies_dataframes_BI.append(df_dummies_BI)\n",
    "    features.append(\n",
    "        [col for col in df_dummies.columns if col not in [\"debate_id\", \"agreed_before\"]]\n",
    "    )\n",
    "    features_BI.append(\n",
    "        [\n",
    "            col\n",
    "            for col in df_dummies_BI.columns\n",
    "            if col not in [\"debate_id\", \"agreed_before\"]\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=20)\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "models = []\n",
    "big_issues = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "\n",
    "\n",
    "for classifier, classifier_name in zip(\n",
    "    [LR_clf, GB_clf], [\"Logistic Regression\", \"Gradient Boosting\"]\n",
    "):\n",
    "    scores = []\n",
    "    scores_BI = []\n",
    "\n",
    "    for i, df in enumerate(issues_dfs):\n",
    "        debate_ids = np.array(list(df.debate_id.unique()))\n",
    "\n",
    "        for train_index, test_index in kf.split(debate_ids):\n",
    "            # get features and outputs\n",
    "            X_train, y_train, X_test, y_test = get_train_test(\n",
    "                train_index, test_index, debate_ids, dummies_dataframes[i], features[i]\n",
    "            )\n",
    "\n",
    "            X_train_BI, y_train_BI, X_test_BI, y_test_BI = get_train_test(\n",
    "                train_index,\n",
    "                test_index,\n",
    "                debate_ids,\n",
    "                dummies_dataframes_BI[i],\n",
    "                features_BI[i],\n",
    "            )\n",
    "\n",
    "            # get scores\n",
    "            score = classifier.fit(X_train, y_train).score(X_test, y_test)\n",
    "            scores.append(score)\n",
    "\n",
    "            score = classifier.fit(X_train_BI, y_train_BI).score(X_test_BI, y_test_BI)\n",
    "            scores_BI.append(score)\n",
    "\n",
    "    accuracy, ci = get_metrics(scores)\n",
    "    accuracy_BI, ci_BI = get_metrics(scores_BI)\n",
    "\n",
    "    models += [classifier_name, classifier_name]\n",
    "    big_issues += [\"No\", \"Yes\"]\n",
    "    accuracies += [accuracy, accuracy_BI]\n",
    "    confidence_intervals += [ci, ci_BI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Big Issues\": big_issues,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "issues_table_complete = pd.concat([issues_table, regression_results])\n",
    "issues_table_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tidy/latex_tables/issues_results.txt\", \"w\") as f:\n",
    "    f.write(\n",
    "        issues_table_complete.to_latex(index=False, float_format=\"%.2f\", position=\"h\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(\n",
    "    q1[q1.model != \"MTurk\"]\n",
    "    .pivot(index=\"debate_id\", columns=\"model\", values=\"processed_gpt_response\")\n",
    "    .dropna()\n",
    ").merge(q1[[\"debate_id\", \"ground_truth\"]].drop_duplicates(), on=\"debate_id\")\n",
    "\n",
    "\n",
    "df[\"ground_truth\"] = df.ground_truth.apply(lambda x: 1 if x == \"Pro\" else x)\n",
    "df[\"ground_truth\"] = df.ground_truth.apply(lambda x: 0 if x == \"Tie\" else x)\n",
    "df[\"ground_truth\"] = df.ground_truth.apply(lambda x: -1 if x == \"Con\" else x)\n",
    "\n",
    "\n",
    "features = [col for col in df.columns if col not in [\"debate_id\", \"ground_truth\"]]\n",
    "\n",
    "kf = KFold(n_splits=20)\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "questions = []\n",
    "\n",
    "for classifier, classifier_name in zip(\n",
    "    [LR_clf, GB_clf], [\"Logistic Regression\", \"Gradient Boosting\"]\n",
    "):\n",
    "    scores = []\n",
    "\n",
    "    debate_ids = np.array(list(df.debate_id.unique()))\n",
    "\n",
    "    for train_index, test_index in kf.split(debate_ids):\n",
    "        # get features and outputs\n",
    "        X_train, y_train, X_test, y_test = get_train_test(\n",
    "            train_index, test_index, debate_ids, df, features, \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        # get scores\n",
    "        score = classifier.fit(X_train, y_train).score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "    accuracy, ci = get_metrics(scores)\n",
    "\n",
    "    questions.append(\"1\")\n",
    "    models.append(classifier_name)\n",
    "    accuracies.append(accuracy)\n",
    "    confidence_intervals.append(ci)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(\n",
    "    q2[q2.model != \"MTurk\"]\n",
    "    .pivot(index=[\"debate_id\", \"voter_id\"], columns=\"model\", values=\"processed_gpt_response\")\n",
    "    .dropna()\n",
    ").merge(q2[[\"debate_id\", \"voter_id\", \"ground_truth\"]].drop_duplicates(), on=[\"debate_id\", \"voter_id\"])\n",
    "\n",
    "df[\"ground_truth\"] = df.ground_truth.apply(lambda x: 1 if x == \"Pro\" else x)\n",
    "df[\"ground_truth\"] = df.ground_truth.apply(lambda x: 0 if x == \"Tie\" else x)\n",
    "df[\"ground_truth\"] = df.ground_truth.apply(lambda x: -1 if x == \"Con\" else x)\n",
    "\n",
    "features = [col for col in df.columns if col not in [\"debate_id\", \"voter_id\", \"ground_truth\"]]\n",
    "\n",
    "kf = KFold(n_splits=20)\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "for classifier, classifier_name in zip(\n",
    "    [LR_clf, GB_clf], [\"Logistic Regression\", \"Gradient Boosting\"]\n",
    "):\n",
    "    scores = []\n",
    "\n",
    "    debate_ids = np.array(list(df.debate_id.unique()))\n",
    "\n",
    "    for train_index, test_index in kf.split(debate_ids):\n",
    "        # get features and outputs\n",
    "        X_train, y_train, X_test, y_test = get_train_test(\n",
    "            train_index, test_index, debate_ids, df, features, \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        # get scores\n",
    "        score = classifier.fit(X_train, y_train).score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "    accuracy, ci = get_metrics(scores)\n",
    "\n",
    "    questions.append(\"2\")\n",
    "    models.append(classifier_name)\n",
    "    accuracies.append(accuracy)\n",
    "    confidence_intervals.append(ci)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(\n",
    "    q3[q3.model != \"MTurk\"]\n",
    "    .pivot(index=[\"debate_id\", \"voter_id\"], columns=\"model\", values=\"processed_gpt_response\")\n",
    "    .dropna()\n",
    ").merge(q3[[\"debate_id\", \"voter_id\", \"ground_truth\"]].drop_duplicates(), on=[\"debate_id\", \"voter_id\"])\n",
    "\n",
    "df[\"ground_truth\"] = df.ground_truth.apply(lambda x: 1 if x == \"Pro\" else x)\n",
    "df[\"ground_truth\"] = df.ground_truth.apply(lambda x: 0 if x == \"Tie\" else x)\n",
    "df[\"ground_truth\"] = df.ground_truth.apply(lambda x: -1 if x == \"Con\" else x)\n",
    "\n",
    "features = [col for col in df.columns if col not in [\"debate_id\", \"voter_id\", \"ground_truth\"]]\n",
    "\n",
    "kf = KFold(n_splits=20)\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "for classifier, classifier_name in zip(\n",
    "    [LR_clf, GB_clf], [\"Logistic Regression\", \"Gradient Boosting\"]\n",
    "):\n",
    "    scores = []\n",
    "\n",
    "    debate_ids = np.array(list(df.debate_id.unique()))\n",
    "\n",
    "    for train_index, test_index in kf.split(debate_ids):\n",
    "        # get features and outputs\n",
    "        X_train, y_train, X_test, y_test = get_train_test(\n",
    "            train_index, test_index, debate_ids, df, features, \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        # get scores\n",
    "        score = classifier.fit(X_train, y_train).score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "    accuracy, ci = get_metrics(scores)\n",
    "\n",
    "    questions.append(\"3\")\n",
    "    models.append(classifier_name)\n",
    "    accuracies.append(accuracy)\n",
    "    confidence_intervals.append(ci)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy (\\%)</th>\n",
       "      <th>95\\% Confidence Interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>61.94</td>\n",
       "      <td>(58.54, 65.34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>60.77</td>\n",
       "      <td>(57.31, 64.23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>57.25</td>\n",
       "      <td>(52.59, 61.91)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>56.57</td>\n",
       "      <td>(52.2, 60.94)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>54.75</td>\n",
       "      <td>(49.84, 59.65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>53.75</td>\n",
       "      <td>(49.31, 58.19)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Question                Model  Accuracy (\\%) 95\\% Confidence Interval\n",
       "0        1  Logistic Regression          61.94           (58.54, 65.34)\n",
       "1        1    Gradient Boosting          60.77           (57.31, 64.23)\n",
       "2        2  Logistic Regression          57.25           (52.59, 61.91)\n",
       "3        2    Gradient Boosting          56.57            (52.2, 60.94)\n",
       "4        3  Logistic Regression          54.75           (49.84, 59.65)\n",
       "5        3    Gradient Boosting          53.75           (49.31, 58.19)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_model = pd.DataFrame(\n",
    "    {\n",
    "        \"Question\": questions,\n",
    "        \"Model\": models,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "stacked_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tidy/latex_tables/stacked_model.txt\", \"w\") as f:\n",
    "    f.write(\n",
    "        stacked_model.to_latex(index=False, float_format=\"%.2f\", position=\"h\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
