{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/paularescala/Documents/Professional/Masters-Thesis-2023/debate-gpt\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "from debate_gpt.results_analysis.analysis_helpers import get_train_test, get_metrics, get_bootstrap, calculate_cohens_kappa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = pd.read_json(\"data/tidy/llm_outputs/q1.json\")\n",
    "q2 = pd.read_json(\"data/tidy/llm_outputs/q2.json\")\n",
    "q3 = pd.read_json(\"data/tidy/llm_outputs/q3.json\")\n",
    "binary = pd.read_json(\"data/tidy/llm_outputs/q2-binary.json\")\n",
    "issues = pd.read_json(\"data/tidy/llm_outputs/q2-issues.json\")\n",
    "votes_df = pd.read_json(\"data/processing/processed_data/votes_df.json\")\n",
    "\n",
    "with open(\"data/tidy/datasets/datasets.json\") as f:\n",
    "    dataset_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = list(q1.model.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iaa_df = issues.pivot(\n",
    "    index=[\"debate_id\", \"voter_id\"],\n",
    "    columns=[\"model\", \"big_issues\", \"reasoning\"],\n",
    "    values=\"processed_gpt_response\",\n",
    ")\n",
    "names = []\n",
    "for column in iaa_df.columns:\n",
    "    name = column[0] + (\"-bi\" if column[1] else \"\") + (\"-r\" if column[2] else \"\")\n",
    "    names.append(name)\n",
    "\n",
    "iaa_df.columns = names\n",
    "iaa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names1 = []\n",
    "names2 = []\n",
    "kappas = []\n",
    "iaa_df.columns\n",
    "for column1 in iaa_df.columns:\n",
    "    for column2 in iaa_df.columns:\n",
    "        tmp_df = iaa_df[[column1, column2]]\n",
    "        kappa, _, _ = calculate_cohens_kappa(tmp_df)\n",
    "\n",
    "        names1.append(column1)\n",
    "        names2.append(column2)\n",
    "        kappas.append(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_df = pd.DataFrame({\"model1\": names1, \"model2\": names2, \"kappa\": kappas}).pivot(\n",
    "    index=\"model1\", columns=\"model2\", values=\"kappa\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(kappa_df, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for df, question in zip([q1, q2, q3], [\"q1\", \"q2\", \"q3\"]):\n",
    "    df[\"correct_form\"] = df.processed_gpt_response == df.gpt_response\n",
    "    df[\"answer_extracted\"] = df.processed_gpt_response.isin([\"Pro\", \"Con\", \"Tie\"])\n",
    "    df = df.groupby(\"model\")[[\"correct_form\", \"answer_extracted\"]].mean().reset_index()\n",
    "    df[\"question\"] = question\n",
    "    df[\"correct_form\"] = df.correct_form * 100\n",
    "    df[\"answer_extracted\"] = df.answer_extracted * 100\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_extraction_df = pd.concat(dfs)[\n",
    "    [\"question\", \"model\", \"correct_form\", \"answer_extracted\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tidy/latex_tables/answer_extraction.txt\", \"w\") as f:\n",
    "    f.write(\n",
    "        answer_extraction_df.to_latex(index=False, float_format=\"%.2f\", position=\"h\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "datasets = []\n",
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "\n",
    "for df, question in zip([q1, q2, q3], [\"1\", \"2\", \"3\"]):\n",
    "    for dataset in [\"Trimmed\", \"Issues\"]:\n",
    "        for model in model_list:\n",
    "            if (model == \"MTurk\") and (dataset == \"Trimmed\"):\n",
    "                continue\n",
    "            model_df = df[df.model == model]\n",
    "            temp_df = model_df[model_df.debate_id.isin(dataset_dict[dataset])]\n",
    "            accuracy, _, _, ci = get_bootstrap(temp_df)\n",
    "\n",
    "            questions.append(question)\n",
    "            models.append(model)\n",
    "            datasets.append(dataset)\n",
    "            accuracies.append(accuracy)\n",
    "            confidence_intervals.append(ci)\n",
    "\n",
    "            if (question == \"1\") & (model == \"llama\"):\n",
    "                voter_agg_df = (\n",
    "                    votes_df[[\"debate_id\", \"voter_id\", \"more_convincing_arguments\"]]\n",
    "                    .merge(\n",
    "                        temp_df.groupby(\"debate_id\")\n",
    "                        .ground_truth.first()\n",
    "                        .to_frame()\n",
    "                        .reset_index(),\n",
    "                        on=\"debate_id\",\n",
    "                    )\n",
    "                    .dropna()\n",
    "                )\n",
    "                voter_agg = (\n",
    "                    (\n",
    "                        voter_agg_df.more_convincing_arguments\n",
    "                        == voter_agg_df.ground_truth\n",
    "                    ).sum()\n",
    "                    / len(voter_agg_df)\n",
    "                    * 100\n",
    "                )\n",
    "\n",
    "                # get confidendce interval\n",
    "                stats = []\n",
    "                for _ in range(1000):\n",
    "                    temp_df = voter_agg_df.sample(len(voter_agg_df), replace=True)\n",
    "                    stats.append(\n",
    "                        (\n",
    "                            temp_df.more_convincing_arguments == temp_df.ground_truth\n",
    "                        ).sum()\n",
    "                        / len(temp_df)\n",
    "                        * 100\n",
    "                    )\n",
    "\n",
    "                questions.append(question)\n",
    "                models.append(\"VoterAgg\")\n",
    "                datasets.append(dataset)\n",
    "                accuracies.append(voter_agg)\n",
    "                confidence_intervals.append(\n",
    "                    (round(sorted(stats)[25], 2), round(sorted(stats)[975], 2))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Question\": questions,\n",
    "        \"Model\": models,\n",
    "        \"Dataset\": datasets,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "primary_table[\"Model\"] = (\n",
    "    primary_table.Model.str.capitalize()\n",
    "    .str.replace(\"Gpt-\", \"GPT-\")\n",
    "    .str.replace(\"Mturk\", \"MTurk\")\n",
    "    .str.replace(\"Voteragg\", \"VoterAgg\")\n",
    ")\n",
    "primary_table[\"Model\"] = pd.Categorical(\n",
    "    primary_table[\"Model\"],\n",
    "    [\"Llama\", \"Mistral\", \"GPT-3.5\", \"GPT-4\", \"VoterAgg\", \"MTurk\"],\n",
    ")\n",
    "primary_table[\"Dataset\"] = pd.Categorical(\n",
    "    primary_table[\"Dataset\"], [\"Trimmed\", \"Short\", \"Issues\"]\n",
    ")\n",
    "primary_table = primary_table.sort_values([\"Question\", \"Dataset\", \"Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"latex_tables/primary_results.txt\", \"w\") as f:\n",
    "#     f.write(primary_table.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "models = []\n",
    "types = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "\n",
    "for dataset in [\"Trimmed\", \"Issues\"]:\n",
    "    for model in model_list:\n",
    "        model_df = binary[binary.model == model]\n",
    "        temp_df = model_df[model_df.debate_id.isin(dataset_dict[dataset])]\n",
    "        accuracy, _, _, ci = get_bootstrap(temp_df)\n",
    "\n",
    "        models.append(model)\n",
    "        datasets.append(dataset)\n",
    "        accuracies.append(accuracy)\n",
    "        confidence_intervals.append(ci)\n",
    "        types.append(\"Binary\")\n",
    "\n",
    "        model_df = q2[q2.model == model]\n",
    "        temp_df = model_df[model_df.debate_id.isin(list(temp_df.debate_id.unique()))]\n",
    "        accuracy, _, _, ci = get_bootstrap(temp_df)\n",
    "        models.append(model)\n",
    "        datasets.append(dataset)\n",
    "        accuracies.append(accuracy)\n",
    "        confidence_intervals.append(ci)\n",
    "        types.append(\"3-class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Dataset\": datasets,\n",
    "        \"Classes\": types,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "binary_table[\"Model\"] = (\n",
    "    binary_table.Model.str.capitalize()\n",
    "    .str.replace(\"Gpt-\", \"GPT-\")\n",
    "    .str.replace(\"Mturk\", \"MTurk\")\n",
    ")\n",
    "binary_table[\"Model\"] = pd.Categorical(\n",
    "    binary_table[\"Model\"], [\"Llama\", \"Mistral\", \"GPT-3.5\", \"GPT-4\", \"MTurk\"]\n",
    ")\n",
    "binary_table[\"Dataset\"] = pd.Categorical(\n",
    "    binary_table[\"Dataset\"], [\"Trimmed\", \"Short\", \"Issues\"]\n",
    ")\n",
    "binary_table = binary_table.sort_values([\"Dataset\", \"Classes\", \"Model\"])\n",
    "binary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tidy/latex_tables/binary_results.txt\", \"w\") as f:\n",
    "    f.write(binary_table.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues[\"big_issues\"] = issues.apply(\n",
    "    lambda x: False if x.model == \"MTurk\" else x.big_issues, axis=1\n",
    ")\n",
    "issues[\"reasoning\"] = issues.apply(\n",
    "    lambda x: True if x.model == \"MTurk\" else x.reasoning, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "big_issues = []\n",
    "reasoning = []\n",
    "\n",
    "for model in model_list:\n",
    "    for bi in [True, False]:\n",
    "        for r in [True, False]:\n",
    "            model_df = issues[issues.model == model]\n",
    "            temp_df = model_df[(model_df.reasoning == r) & (model_df.big_issues == bi)]\n",
    "            accuracy, _, _, ci = get_bootstrap(temp_df)\n",
    "\n",
    "            models.append(model)\n",
    "            accuracies.append(accuracy)\n",
    "            confidence_intervals.append(ci)\n",
    "            big_issues.append(bi)\n",
    "            reasoning.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Big Issues\": big_issues,\n",
    "        \"Reasoning\": reasoning,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "issues_table[\"Model\"] = (\n",
    "    issues_table.Model.str.capitalize()\n",
    "    .str.replace(\"Gpt-\", \"GPT-\")\n",
    "    .str.replace(\"Mturk\", \"MTurk\")\n",
    ")\n",
    "issues_table[\"Model\"] = pd.Categorical(\n",
    "    issues_table[\"Model\"], [\"Llama\", \"Mistral\", \"GPT-3.5\", \"GPT-4\", \"MTurk\"]\n",
    ")\n",
    "issues_table = issues_table.sort_values([\"Model\", \"Big Issues\", \"Reasoning\"])\n",
    "issues_table = issues_table.dropna(subset=[\"Accuracy (\\%)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abortion_df = pd.read_json(\"data/tidy/regression_files/abortion.json\")\n",
    "abortion_BI_df = pd.read_json(\"data/tidy/regression_files/abortion_BI.json\")\n",
    "gay_marriage_df = pd.read_json(\"data/tidy/regression_files/gay_marriage.json\")\n",
    "gay_marriage_BI_df = pd.read_json(\"data/tidy/regression_files/gay_marriage_BI.json\")\n",
    "capital_punishment_df = pd.read_json(\n",
    "    \"data/tidy/regression_files/capital_punishment.json\"\n",
    ")\n",
    "capital_punishment_BI_df = pd.read_json(\n",
    "    \"data/tidy/regression_files/capital_punishment_BI.json\"\n",
    ")\n",
    "issues_dfs = {\n",
    "    \"abortion_df\": abortion_df,\n",
    "    \"abortion_BI_df\": abortion_BI_df,\n",
    "    \"gay_marriage_df\": gay_marriage_df,\n",
    "    \"gay_marriage_BI_df\": gay_marriage_BI_df,\n",
    "    \"capital_punishment_df\": capital_punishment_df,\n",
    "    \"capital_punishment_BI_df\": capital_punishment_BI_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=20)\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "models = []\n",
    "big_issues = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "\n",
    "\n",
    "for classifier, classifier_name in zip(\n",
    "    [LR_clf, GB_clf], [\"Logistic Regression\", \"Gradient Boosting\"]\n",
    "):\n",
    "    scores = []\n",
    "    scores_BI = []\n",
    "\n",
    "    for issue in issues_dfs:\n",
    "        df = issues_dfs[issue]\n",
    "        debate_ids = np.array(list(df.debate_id.unique()))\n",
    "\n",
    "        for train_index, test_index in kf.split(debate_ids):\n",
    "            # get features and outputs\n",
    "            X_train, y_train, X_test, y_test = get_train_test(\n",
    "                train_index, test_index, debate_ids, df\n",
    "            )\n",
    "            if \"BI\" not in issue:\n",
    "                # get scores\n",
    "                score = classifier.fit(X_train, y_train).score(X_test, y_test)\n",
    "                scores.append(score)\n",
    "            else:\n",
    "                score = classifier.fit(X_train, y_train).score(X_test, y_test)\n",
    "                scores_BI.append(score)\n",
    "\n",
    "    accuracy, ci = get_metrics(scores)\n",
    "    accuracy_BI, ci_BI = get_metrics(scores_BI)\n",
    "\n",
    "    models += [classifier_name, classifier_name]\n",
    "    big_issues += [\"No\", \"Yes\"]\n",
    "    accuracies += [accuracy, accuracy_BI]\n",
    "    confidence_intervals += [ci, ci_BI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Big Issues\": big_issues,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "regression_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issues_table_complete = pd.concat([issues_table, regression_results])\n",
    "# issues_table_complete\n",
    "# with open(\"data/tidy/latex_tables/issues_results.txt\", \"w\") as f:\n",
    "#     f.write(\n",
    "#         issues_table_complete.to_latex(index=False, float_format=\"%.2f\", position=\"h\")\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_stacked = pd.read_json(\"data/tidy/regression_files/q1-stacked.json\")\n",
    "q2_stacked = pd.read_json(\"data/tidy/regression_files/q2-stacked.json\")\n",
    "q3_stacked = pd.read_json(\"data/tidy/regression_files/q3-stacked.json\")\n",
    "\n",
    "stacked_dfs = {\"q1\": q1_stacked, \"q2\": q2_stacked, \"q3\": q3_stacked}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=20)\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "questions = []\n",
    "\n",
    "for classifier, classifier_name in zip(\n",
    "    [LR_clf, GB_clf], [\"Logistic Regression\", \"Gradient Boosting\"]\n",
    "):\n",
    "\n",
    "    for question in stacked_dfs:\n",
    "        df = stacked_dfs[question]\n",
    "        debate_ids = np.array(list(df.debate_id.unique()))\n",
    "        scores = []\n",
    "        for train_index, test_index in kf.split(debate_ids):\n",
    "            # get features and outputs\n",
    "            X_train, y_train, X_test, y_test = get_train_test(\n",
    "                train_index, test_index, debate_ids, df, \"ground_truth\"\n",
    "            )\n",
    "\n",
    "            # get scores\n",
    "            score = classifier.fit(X_train, y_train).score(X_test, y_test)\n",
    "            scores.append(score)\n",
    "\n",
    "        accuracy, ci = get_metrics(scores)\n",
    "\n",
    "        questions.append(question)\n",
    "        models.append(classifier_name)\n",
    "        accuracies.append(accuracy)\n",
    "        confidence_intervals.append(ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model = pd.DataFrame(\n",
    "    {\n",
    "        \"Question\": questions,\n",
    "        \"Model\": models,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "stacked_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/tidy/latex_tables/stacked_model.txt\", \"w\") as f:\n",
    "#     f.write(\n",
    "#         stacked_model.to_latex(index=False, float_format=\"%.2f\", position=\"h\")\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abortion_df = pd.read_json(\"data/tidy/regression_files/abortion_BI_stacked.json\")\n",
    "gay_marriage_df = pd.read_json(\n",
    "    \"data/tidy/regression_files/gay_marriage_BI_stacked.json\"\n",
    ")\n",
    "capital_punishment_df = pd.read_json(\n",
    "    \"data/tidy/regression_files/capital_punishment_BI_stacked.json\"\n",
    ")\n",
    "issues_dfs = {\n",
    "    \"abortion_df\": abortion_df,\n",
    "    \"gay_marriage_df\": gay_marriage_df,\n",
    "    \"capital_punishment_df\": capital_punishment_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=20)\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "\n",
    "\n",
    "for classifier, classifier_name in zip(\n",
    "    [LR_clf, GB_clf], [\"Logistic Regression\", \"Gradient Boosting\"]\n",
    "):\n",
    "    scores = []\n",
    "\n",
    "    for issue in issues_dfs:\n",
    "        df = issues_dfs[issue]\n",
    "        debate_ids = np.array(list(df.debate_id.unique()))\n",
    "\n",
    "        for train_index, test_index in kf.split(debate_ids):\n",
    "            # get features and outputs\n",
    "            X_train, y_train, X_test, y_test = get_train_test(\n",
    "                train_index, test_index, debate_ids, df\n",
    "            )\n",
    "\n",
    "            score = classifier.fit(X_train, y_train).score(X_test, y_test)\n",
    "            scores.append(score)\n",
    "\n",
    "    accuracy, ci = get_metrics(scores)\n",
    "    models.append(classifier_name)\n",
    "    accuracies.append(accuracy)\n",
    "    confidence_intervals.append(ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"95\\% Confidence Interval\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "regression_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
