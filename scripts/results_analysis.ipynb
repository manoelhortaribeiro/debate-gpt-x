{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "from helpers.metrics import get_bootstrap\n",
    "from helpers.process_results import (\n",
    "    majority_vote,\n",
    "    process_crowdsourcing_data,\n",
    "    process_gpt_response,\n",
    ")\n",
    "from scipy.stats import t\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Basic Data Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and get statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ground truth dataframe for Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_id</th>\n",
       "      <th>q1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>Tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   debate_id   q1\n",
       "0          0  Con\n",
       "1          1  Con\n",
       "2          3  Pro\n",
       "3          4  Con\n",
       "4          5  Con\n",
       "5          7  Con\n",
       "6          8  Pro\n",
       "7          9  Tie\n",
       "8         10  Pro\n",
       "9         11  Con"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_df = pd.read_json(\"../data/processed_data/votes_df.json\")\n",
    "\n",
    "ground_truth_df = votes_df[[\"debate_id\", \"more_convincing_arguments\"]]\n",
    "ground_truth_df = ground_truth_df.rename(columns={\"more_convincing_arguments\": \"q1\"})\n",
    "ground_truth_df = (\n",
    "    ground_truth_df.groupby(\"debate_id\")\n",
    "    .apply(lambda x: majority_vote(x, \"q1\"))\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"q1\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ground_truth_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all LLM data in one DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>debate_length</th>\n",
       "      <th>model</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>voter_id</th>\n",
       "      <th>response</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>correct_form</th>\n",
       "      <th>answer_extracted</th>\n",
       "      <th>q1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>trimmed</td>\n",
       "      <td>Llama</td>\n",
       "      <td>358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q3</td>\n",
       "      <td>trimmed</td>\n",
       "      <td>Llama</td>\n",
       "      <td>358</td>\n",
       "      <td>imabench</td>\n",
       "      <td>Con</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q3</td>\n",
       "      <td>trimmed</td>\n",
       "      <td>Llama</td>\n",
       "      <td>358</td>\n",
       "      <td>9spaceking</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q2</td>\n",
       "      <td>full</td>\n",
       "      <td>Llama</td>\n",
       "      <td>358</td>\n",
       "      <td>imabench</td>\n",
       "      <td>other</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q2</td>\n",
       "      <td>full</td>\n",
       "      <td>Llama</td>\n",
       "      <td>358</td>\n",
       "      <td>9spaceking</td>\n",
       "      <td>other</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question debate_length  model  debate_id    voter_id response   q2   q3  \\\n",
       "0       q1       trimmed  Llama        358         NaN    other  NaN  NaN   \n",
       "1       q3       trimmed  Llama        358    imabench      Con  Tie  Tie   \n",
       "2       q3       trimmed  Llama        358  9spaceking      Pro  Tie  Tie   \n",
       "3       q2          full  Llama        358    imabench    other  Tie  Tie   \n",
       "4       q2          full  Llama        358  9spaceking    other  Tie  Tie   \n",
       "\n",
       "   correct_form  answer_extracted   q1  \n",
       "0         False             False  Con  \n",
       "1         False              True  Con  \n",
       "2         False              True  Con  \n",
       "3         False             False  Con  \n",
       "4         False             False  Con  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propositions_df = pd.read_json(\"../data/raw_data/propositions.json\")\n",
    "PoliPropDataset = list(propositions_df.debate_id.unique())\n",
    "\n",
    "full_list = []\n",
    "all_results_files = glob.glob(\"../results/*/q*/*q*\")\n",
    "\n",
    "for file in all_results_files:\n",
    "    model = file.split(\"/\")[2]\n",
    "    question = file.split(\"/\")[3]\n",
    "\n",
    "    df = pd.read_json(file)\n",
    "    df[\"model\"] = model\n",
    "    df[\"question\"] = question\n",
    "\n",
    "    full_list.append(df)\n",
    "\n",
    "full_df = pd.concat(full_list)\n",
    "full_df = full_df[full_df.debate_id.isin(PoliPropDataset)] \n",
    "full_df = process_gpt_response(full_df)\n",
    "\n",
    "full_df = full_df[\n",
    "    [\n",
    "        \"question\",\n",
    "        \"debate_length\",\n",
    "        \"model\",\n",
    "        \"debate_id\",\n",
    "        \"voter_id\",\n",
    "        \"gpt_response\",\n",
    "        \"agreed_before\", \n",
    "        \"agreed_after\", \n",
    "        \"correct_form\",\n",
    "        \"answer_extracted\",\n",
    "    ]\n",
    "]\n",
    "full_df = full_df.rename(\n",
    "    columns={\n",
    "        \"gpt_response\": \"response\",\n",
    "        \"agreed_before\": \"q2\",\n",
    "        \"agreed_after\": \"q3\",\n",
    "    }\n",
    ")\n",
    "full_df = full_df.merge(ground_truth_df, on=\"debate_id\")\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring all the models and questions have the same debate ids and voter ids\n",
    "# display(full_df.groupby([\"question\", \"model\"]).debate_id.nunique()) # UNCOMMENT to view\n",
    "# display(full_df.groupby([\"question\", \"model\"]).voter_id.nunique()) # UNCOMMENT line to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_debates = full_df[full_df.debate_length == \"full\"].groupby([\"question\", \"model\"]).debate_id.unique()\n",
    "\n",
    "current_set = set(short_debates[0])\n",
    "for sds in short_debates:\n",
    "    current_set = current_set.intersection(set(sds))\n",
    "\n",
    "SHORT = list(current_set)\n",
    "TRIMMED = PoliPropDataset\n",
    "ABORTION = list(\n",
    "    propositions_df[\n",
    "        (propositions_df.proposition.str.lower().str.contains(\"abortion\"))\n",
    "    ].debate_id.unique()\n",
    ")\n",
    "GAY_MARRIAGE = list(propositions_df[\n",
    "    (\n",
    "        propositions_df.proposition.str.lower().str.contains(\"same sex\")\n",
    "        | propositions_df.proposition.str.lower().str.contains(\"gay\")\n",
    "        | propositions_df.proposition.str.lower().str.contains(\"same-sex\")\n",
    "    )\n",
    "    & (propositions_df.proposition.str.lower().str.contains(\"marriage\"))\n",
    "].debate_id.unique())\n",
    "\n",
    "CAPITAL_PUNISHMENT = list(propositions_df[\n",
    "    (\n",
    "        propositions_df.proposition.str.lower().str.contains(\"death penalty\")\n",
    "        | propositions_df.proposition.str.lower().str.contains(\"capital punishment\")\n",
    "    )\n",
    "].debate_id.unique())\n",
    "\n",
    "ISSUES = list(set(ABORTION + GAY_MARRIAGE + CAPITAL_PUNISHMENT))\n",
    "\n",
    "DATASETS  = [TRIMMED, SHORT, ISSUES]\n",
    "DATASET_NAMES  = [\"Trimmed\", \"Short\", \"Issues\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Number of debates</th>\n",
       "      <th>Number of votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trimmed</td>\n",
       "      <td>852</td>\n",
       "      <td>4871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Short</td>\n",
       "      <td>276</td>\n",
       "      <td>1538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Issues</td>\n",
       "      <td>127</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset  Number of debates  Number of votes\n",
       "0  Trimmed                852             4871\n",
       "1    Short                276             1538\n",
       "2   Issues                127              836"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_filtered_df = pd.read_json(\"../data/filtered_data/votes_filtered_df.json\")\n",
    "\n",
    "num_debates = []\n",
    "num_votes = []\n",
    "for debate_ids in [TRIMMED, SHORT, ISSUES]:\n",
    "    num_debates.append(len(debate_ids))\n",
    "    num_votes.append(\n",
    "        (len(votes_filtered_df[votes_filtered_df.debate_id.isin(debate_ids)]))\n",
    "    )\n",
    "\n",
    "datasets_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Dataset\": DATASET_NAMES,\n",
    "        \"Number of debates\": num_debates,\n",
    "        \"Number of votes\": num_votes,\n",
    "    }\n",
    ")\n",
    "datasets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare MTurk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_id</th>\n",
       "      <th>voter_id</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   debate_id voter_id question response   q1    q2    q3\n",
       "0        706      NaN       q1      Con  Pro  None  None\n",
       "1        706      NaN       q1      Con  Pro  None  None\n",
       "2        706      NaN       q1      Pro  Pro  None  None\n",
       "3        706      NaN       q1      Con  Pro  None  None\n",
       "4        706      NaN       q1      Con  Pro  None  None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all crowdsourcing files\n",
    "crowd_files = glob.glob(\"../data/raw_data/crowd/*\")\n",
    "crowd_dfs = []\n",
    "for file in crowd_files:\n",
    "    crowd_df = pd.read_csv(file)\n",
    "    crowd_dfs.append(crowd_df)\n",
    "\n",
    "# create one dataframe of all crowdsourcing data\n",
    "crowd_df = pd.concat(crowd_dfs).reset_index(drop=True)\n",
    "crowd_df = process_crowdsourcing_data(crowd_df)\n",
    "crowd_df = crowd_df.groupby([\"debate_id\", \"voter_id\"]).sample(1)\n",
    "crowd_df = pd.melt(\n",
    "    crowd_df,\n",
    "    id_vars=[\"debate_id\", \"voter_id\"],\n",
    "    value_vars=[\"q1\", \"q2\", \"q3\"],\n",
    "    var_name=\"question\",\n",
    "    value_name=\"response\",\n",
    ")\n",
    "\n",
    "crowd_df[\"voter_id\"] = crowd_df.apply(\n",
    "    lambda x: np.nan if x.question == \"q1\" else x.voter_id, axis=1\n",
    ")\n",
    "crowd_df = crowd_df.merge(\n",
    "    full_df.groupby([\"question\",\"debate_id\", \"voter_id\"], dropna=False)\n",
    "    .first()\n",
    "    .reset_index()[[\"question\", \"debate_id\", \"voter_id\", \"q1\", \"q2\", \"q3\"]],\n",
    "    on=[\"question\", \"debate_id\", \"voter_id\"],\n",
    ")\n",
    "\n",
    "crowd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get table for correct form and answer extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\begin{tabular}{llrr}\n",
      "\\toprule\n",
      "question & model & correct_form & answer_extracted \\\\\n",
      "\\midrule\n",
      "q1 & GPT-3.5 & 99.88 & 100.00 \\\\\n",
      "q1 & GPT-4 & 99.06 & 100.00 \\\\\n",
      "q1 & Llama & 0.00 & 94.95 \\\\\n",
      "q1 & Mistral & 62.79 & 95.07 \\\\\n",
      "q2 & GPT-3.5 & 99.84 & 99.88 \\\\\n",
      "q2 & GPT-4 & 100.00 & 100.00 \\\\\n",
      "q2 & Llama & 0.00 & 97.13 \\\\\n",
      "q2 & Mistral & 67.13 & 100.00 \\\\\n",
      "q3 & GPT-3.5 & 99.82 & 99.94 \\\\\n",
      "q3 & GPT-4 & 99.61 & 99.98 \\\\\n",
      "q3 & Llama & 0.00 & 91.50 \\\\\n",
      "q3 & Mistral & 17.22 & 79.72 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_form_df = (\n",
    "    full_df.groupby([\"question\", \"model\"])[[\"correct_form\", \"answer_extracted\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "correct_form_df[[\"correct_form\", \"answer_extracted\"]] = (\n",
    "    correct_form_df[[\"correct_form\", \"answer_extracted\"]] * 100\n",
    ")\n",
    "\n",
    "print(correct_form_df.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the accuracies for each configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "datasets = []\n",
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "pro_recalls = []\n",
    "con_recalls = []\n",
    "\n",
    "for question in full_df.question.unique():\n",
    "    accuracy, recalls, _, ci = get_bootstrap(\n",
    "        crowd_df[(crowd_df.debate_id.isin(ISSUES)) & (crowd_df.question == question)],\n",
    "        question,\n",
    "    )\n",
    "\n",
    "    questions.append(question)\n",
    "    datasets.append(\"Issues\")\n",
    "    models.append(\"MTurk\")\n",
    "    accuracies.append(accuracy)\n",
    "    confidence_intervals.append(ci)\n",
    "    pro_recalls.append(recalls[0])\n",
    "    con_recalls.append(recalls[1])\n",
    "\n",
    "    for model in full_df.model.unique():\n",
    "        for dataset, name in zip(\n",
    "            [TRIMMED, SHORT, ISSUES], [\"Trimmed\", \"Short\", \"Issues\"]\n",
    "        ):\n",
    "\n",
    "            temp_df = full_df[\n",
    "                (full_df.question == question)\n",
    "                & (full_df.model == model)\n",
    "                & (full_df.debate_id.isin(dataset))\n",
    "            ]\n",
    "            accuracy, recalls, _, ci = get_bootstrap(temp_df, question)\n",
    "\n",
    "            questions.append(question)\n",
    "            datasets.append(name)\n",
    "            models.append(model)\n",
    "            accuracies.append(accuracy)\n",
    "            confidence_intervals.append(ci)\n",
    "            pro_recalls.append(recalls[0])\n",
    "            con_recalls.append(recalls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\begin{tabular}{lllrrrl}\n",
      "\\toprule\n",
      "Q & Dataset & Model & Pro Recall (\\%) & Con Recall (\\%) & Accuracy (\\%) & Accuracy CI (95\\%) \\\\\n",
      "\\midrule\n",
      "q1 & Trimmed & GPT-3.5 & 69.02 & 30.86 & 43.08 & [34.0, 53.0] \\\\\n",
      "q1 & Trimmed & GPT-4 & 55.83 & 77.49 & 61.03 & [51.0, 71.0] \\\\\n",
      "q1 & Trimmed & Llama & 50.31 & 0.23 & 23.94 & [16.0, 33.0] \\\\\n",
      "q1 & Trimmed & Mistral & 74.54 & 12.99 & 36.38 & [27.0, 46.0] \\\\\n",
      "q1 & Short & GPT-3.5 & 64.29 & 27.70 & 39.13 & [29.0, 49.0] \\\\\n",
      "q1 & Short & GPT-4 & 57.14 & 77.03 & 61.96 & [52.0, 71.0] \\\\\n",
      "q1 & Short & Llama & 35.71 & 0.00 & 17.75 & [11.0, 26.0] \\\\\n",
      "q1 & Short & Mistral & 73.47 & 20.27 & 39.13 & [30.0, 49.0] \\\\\n",
      "q1 & Issues & GPT-3.5 & 69.09 & 29.03 & 44.09 & [35.0, 54.0] \\\\\n",
      "q1 & Issues & GPT-4 & 69.09 & 66.13 & 62.20 & [52.0, 71.0] \\\\\n",
      "q1 & Issues & Llama & 47.27 & 0.00 & 24.41 & [16.0, 33.0] \\\\\n",
      "q1 & Issues & Mistral & 70.91 & 9.68 & 37.01 & [27.0, 46.0] \\\\\n",
      "q1 & Issues & MTurk & 56.34 & 39.69 & 44.19 & [34.0, 54.0] \\\\\n",
      "q2 & Trimmed & GPT-3.5 & 28.51 & 47.20 & 39.36 & [30.0, 49.0] \\\\\n",
      "q2 & Trimmed & GPT-4 & 58.91 & 66.28 & 34.18 & [25.0, 43.0] \\\\\n",
      "q2 & Trimmed & Llama & 24.63 & 0.62 & 49.25 & [39.0, 59.0] \\\\\n",
      "q2 & Trimmed & Mistral & 23.69 & 13.58 & 46.97 & [38.0, 57.0] \\\\\n",
      "q2 & Short & GPT-3.5 & 25.25 & 43.53 & 37.52 & [28.0, 48.0] \\\\\n",
      "q2 & Short & GPT-4 & 54.46 & 70.50 & 26.79 & [18.0, 37.0] \\\\\n",
      "q2 & Short & Llama & 22.28 & 1.08 & 54.42 & [44.0, 63.0] \\\\\n",
      "q2 & Short & Mistral & 23.27 & 13.31 & 51.56 & [42.0, 61.0] \\\\\n",
      "q2 & Issues & GPT-3.5 & 28.37 & 67.07 & 41.87 & [32.0, 51.0] \\\\\n",
      "q2 & Issues & GPT-4 & 67.79 & 73.17 & 42.82 & [33.0, 52.0] \\\\\n",
      "q2 & Issues & Llama & 35.10 & 0.81 & 41.27 & [32.0, 50.0] \\\\\n",
      "q2 & Issues & Mistral & 21.15 & 15.04 & 41.39 & [32.0, 51.0] \\\\\n",
      "q2 & Issues & MTurk & 63.53 & 66.82 & 39.32 & [30.0, 49.0] \\\\\n",
      "q3 & Trimmed & GPT-3.5 & 58.68 & 51.08 & 29.11 & [20.0, 38.0] \\\\\n",
      "q3 & Trimmed & GPT-4 & 62.09 & 78.41 & 33.48 & [24.0, 43.0] \\\\\n",
      "q3 & Trimmed & Llama & 41.42 & 59.30 & 27.49 & [19.0, 36.0] \\\\\n",
      "q3 & Trimmed & Mistral & 33.50 & 44.68 & 26.07 & [18.0, 35.0] \\\\\n",
      "q3 & Short & GPT-3.5 & 61.81 & 52.16 & 21.85 & [14.0, 30.0] \\\\\n",
      "q3 & Short & GPT-4 & 58.79 & 78.74 & 23.73 & [16.0, 32.0] \\\\\n",
      "q3 & Short & Llama & 48.74 & 61.46 & 23.34 & [15.0, 32.0] \\\\\n",
      "q3 & Short & Mistral & 30.65 & 46.18 & 25.23 & [17.0, 34.0] \\\\\n",
      "q3 & Issues & GPT-3.5 & 61.40 & 61.39 & 38.88 & [29.0, 49.0] \\\\\n",
      "q3 & Issues & GPT-4 & 73.02 & 78.38 & 44.38 & [35.0, 54.0] \\\\\n",
      "q3 & Issues & Llama & 59.53 & 63.71 & 38.88 & [30.0, 49.0] \\\\\n",
      "q3 & Issues & Mistral & 42.79 & 52.51 & 34.45 & [25.0, 44.0] \\\\\n",
      "q3 & Issues & MTurk & 68.18 & 65.93 & 39.86 & [29.0, 50.0] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"Q\": questions,\n",
    "        \"Dataset\": datasets,\n",
    "        \"Model\": models,\n",
    "        \"Pro Recall (\\%)\": pro_recalls,\n",
    "        \"Con Recall (\\%)\": con_recalls,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"Accuracy CI (95\\%)\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "results[\"Dataset\"] = pd.Categorical(\n",
    "    results[\"Dataset\"], [\"Trimmed\", \"Short\", \"Issues\"]\n",
    ")\n",
    "results[\"Model\"] = pd.Categorical(\n",
    "    results[\"Model\"], [\"GPT-3.5\", \"GPT-4\", \"Llama\", \"Mistral\", \"MTurk\"]\n",
    ")\n",
    "results = results.sort_values([\"Q\", \"Dataset\", \"Model\"])\n",
    "\n",
    "print(\n",
    "    results.to_latex(\n",
    "        index=False, float_format=\"%.2f\", position=\"h\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>Pro Recall (\\%)</th>\n",
       "      <th>Con Recall (\\%)</th>\n",
       "      <th>Accuracy (\\%)</th>\n",
       "      <th>Accuracy CI (95\\%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>q1</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>69.02</td>\n",
       "      <td>30.86</td>\n",
       "      <td>43.08</td>\n",
       "      <td>[34.0, 53.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q1</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>55.83</td>\n",
       "      <td>77.49</td>\n",
       "      <td>61.03</td>\n",
       "      <td>[51.0, 71.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q1</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>Llama</td>\n",
       "      <td>50.31</td>\n",
       "      <td>0.23</td>\n",
       "      <td>23.94</td>\n",
       "      <td>[16.0, 33.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q1</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>74.54</td>\n",
       "      <td>12.99</td>\n",
       "      <td>36.38</td>\n",
       "      <td>[27.0, 46.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>q1</td>\n",
       "      <td>Short</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>64.29</td>\n",
       "      <td>27.70</td>\n",
       "      <td>39.13</td>\n",
       "      <td>[29.0, 49.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q1</td>\n",
       "      <td>Short</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>57.14</td>\n",
       "      <td>77.03</td>\n",
       "      <td>61.96</td>\n",
       "      <td>[52.0, 71.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q1</td>\n",
       "      <td>Short</td>\n",
       "      <td>Llama</td>\n",
       "      <td>35.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.75</td>\n",
       "      <td>[11.0, 26.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q1</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>73.47</td>\n",
       "      <td>20.27</td>\n",
       "      <td>39.13</td>\n",
       "      <td>[30.0, 49.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>q1</td>\n",
       "      <td>Issues</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>69.09</td>\n",
       "      <td>29.03</td>\n",
       "      <td>44.09</td>\n",
       "      <td>[35.0, 54.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q1</td>\n",
       "      <td>Issues</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>69.09</td>\n",
       "      <td>66.13</td>\n",
       "      <td>62.20</td>\n",
       "      <td>[52.0, 71.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q1</td>\n",
       "      <td>Issues</td>\n",
       "      <td>Llama</td>\n",
       "      <td>47.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.41</td>\n",
       "      <td>[16.0, 33.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q1</td>\n",
       "      <td>Issues</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>70.91</td>\n",
       "      <td>9.68</td>\n",
       "      <td>37.01</td>\n",
       "      <td>[27.0, 46.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>Issues</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>56.34</td>\n",
       "      <td>39.69</td>\n",
       "      <td>44.19</td>\n",
       "      <td>[34.0, 54.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>q2</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>28.51</td>\n",
       "      <td>47.20</td>\n",
       "      <td>39.36</td>\n",
       "      <td>[30.0, 49.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>q2</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>58.91</td>\n",
       "      <td>66.28</td>\n",
       "      <td>34.18</td>\n",
       "      <td>[25.0, 43.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>q2</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>Llama</td>\n",
       "      <td>24.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>49.25</td>\n",
       "      <td>[39.0, 59.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>q2</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>23.69</td>\n",
       "      <td>13.58</td>\n",
       "      <td>46.97</td>\n",
       "      <td>[38.0, 57.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>q2</td>\n",
       "      <td>Short</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>25.25</td>\n",
       "      <td>43.53</td>\n",
       "      <td>37.52</td>\n",
       "      <td>[28.0, 48.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>q2</td>\n",
       "      <td>Short</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>54.46</td>\n",
       "      <td>70.50</td>\n",
       "      <td>26.79</td>\n",
       "      <td>[18.0, 37.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>q2</td>\n",
       "      <td>Short</td>\n",
       "      <td>Llama</td>\n",
       "      <td>22.28</td>\n",
       "      <td>1.08</td>\n",
       "      <td>54.42</td>\n",
       "      <td>[44.0, 63.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>q2</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>23.27</td>\n",
       "      <td>13.31</td>\n",
       "      <td>51.56</td>\n",
       "      <td>[42.0, 61.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>q2</td>\n",
       "      <td>Issues</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>28.37</td>\n",
       "      <td>67.07</td>\n",
       "      <td>41.87</td>\n",
       "      <td>[32.0, 51.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>q2</td>\n",
       "      <td>Issues</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>67.79</td>\n",
       "      <td>73.17</td>\n",
       "      <td>42.82</td>\n",
       "      <td>[33.0, 52.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>q2</td>\n",
       "      <td>Issues</td>\n",
       "      <td>Llama</td>\n",
       "      <td>35.10</td>\n",
       "      <td>0.81</td>\n",
       "      <td>41.27</td>\n",
       "      <td>[32.0, 50.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>q2</td>\n",
       "      <td>Issues</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>21.15</td>\n",
       "      <td>15.04</td>\n",
       "      <td>41.39</td>\n",
       "      <td>[32.0, 51.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>q2</td>\n",
       "      <td>Issues</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>63.53</td>\n",
       "      <td>66.82</td>\n",
       "      <td>39.32</td>\n",
       "      <td>[30.0, 49.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>q3</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>58.68</td>\n",
       "      <td>51.08</td>\n",
       "      <td>29.11</td>\n",
       "      <td>[20.0, 38.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>q3</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>62.09</td>\n",
       "      <td>78.41</td>\n",
       "      <td>33.48</td>\n",
       "      <td>[24.0, 43.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>q3</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>Llama</td>\n",
       "      <td>41.42</td>\n",
       "      <td>59.30</td>\n",
       "      <td>27.49</td>\n",
       "      <td>[19.0, 36.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>q3</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>33.50</td>\n",
       "      <td>44.68</td>\n",
       "      <td>26.07</td>\n",
       "      <td>[18.0, 35.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>q3</td>\n",
       "      <td>Short</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>61.81</td>\n",
       "      <td>52.16</td>\n",
       "      <td>21.85</td>\n",
       "      <td>[14.0, 30.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>q3</td>\n",
       "      <td>Short</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>58.79</td>\n",
       "      <td>78.74</td>\n",
       "      <td>23.73</td>\n",
       "      <td>[16.0, 32.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>q3</td>\n",
       "      <td>Short</td>\n",
       "      <td>Llama</td>\n",
       "      <td>48.74</td>\n",
       "      <td>61.46</td>\n",
       "      <td>23.34</td>\n",
       "      <td>[15.0, 32.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>q3</td>\n",
       "      <td>Short</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>30.65</td>\n",
       "      <td>46.18</td>\n",
       "      <td>25.23</td>\n",
       "      <td>[17.0, 34.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>q3</td>\n",
       "      <td>Issues</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>61.40</td>\n",
       "      <td>61.39</td>\n",
       "      <td>38.88</td>\n",
       "      <td>[29.0, 49.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>q3</td>\n",
       "      <td>Issues</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>73.02</td>\n",
       "      <td>78.38</td>\n",
       "      <td>44.38</td>\n",
       "      <td>[35.0, 54.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>q3</td>\n",
       "      <td>Issues</td>\n",
       "      <td>Llama</td>\n",
       "      <td>59.53</td>\n",
       "      <td>63.71</td>\n",
       "      <td>38.88</td>\n",
       "      <td>[30.0, 49.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>q3</td>\n",
       "      <td>Issues</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>42.79</td>\n",
       "      <td>52.51</td>\n",
       "      <td>34.45</td>\n",
       "      <td>[25.0, 44.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>q3</td>\n",
       "      <td>Issues</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>68.18</td>\n",
       "      <td>65.93</td>\n",
       "      <td>39.86</td>\n",
       "      <td>[29.0, 50.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Q  Dataset    Model  Pro Recall (\\%)  Con Recall (\\%)  Accuracy (\\%)  \\\n",
       "10  q1  Trimmed  GPT-3.5            69.02            30.86          43.08   \n",
       "7   q1  Trimmed    GPT-4            55.83            77.49          61.03   \n",
       "1   q1  Trimmed    Llama            50.31             0.23          23.94   \n",
       "4   q1  Trimmed  Mistral            74.54            12.99          36.38   \n",
       "11  q1    Short  GPT-3.5            64.29            27.70          39.13   \n",
       "8   q1    Short    GPT-4            57.14            77.03          61.96   \n",
       "2   q1    Short    Llama            35.71             0.00          17.75   \n",
       "5   q1    Short  Mistral            73.47            20.27          39.13   \n",
       "12  q1   Issues  GPT-3.5            69.09            29.03          44.09   \n",
       "9   q1   Issues    GPT-4            69.09            66.13          62.20   \n",
       "3   q1   Issues    Llama            47.27             0.00          24.41   \n",
       "6   q1   Issues  Mistral            70.91             9.68          37.01   \n",
       "0   q1   Issues    MTurk            56.34            39.69          44.19   \n",
       "36  q2  Trimmed  GPT-3.5            28.51            47.20          39.36   \n",
       "33  q2  Trimmed    GPT-4            58.91            66.28          34.18   \n",
       "27  q2  Trimmed    Llama            24.63             0.62          49.25   \n",
       "30  q2  Trimmed  Mistral            23.69            13.58          46.97   \n",
       "37  q2    Short  GPT-3.5            25.25            43.53          37.52   \n",
       "34  q2    Short    GPT-4            54.46            70.50          26.79   \n",
       "28  q2    Short    Llama            22.28             1.08          54.42   \n",
       "31  q2    Short  Mistral            23.27            13.31          51.56   \n",
       "38  q2   Issues  GPT-3.5            28.37            67.07          41.87   \n",
       "35  q2   Issues    GPT-4            67.79            73.17          42.82   \n",
       "29  q2   Issues    Llama            35.10             0.81          41.27   \n",
       "32  q2   Issues  Mistral            21.15            15.04          41.39   \n",
       "26  q2   Issues    MTurk            63.53            66.82          39.32   \n",
       "23  q3  Trimmed  GPT-3.5            58.68            51.08          29.11   \n",
       "20  q3  Trimmed    GPT-4            62.09            78.41          33.48   \n",
       "14  q3  Trimmed    Llama            41.42            59.30          27.49   \n",
       "17  q3  Trimmed  Mistral            33.50            44.68          26.07   \n",
       "24  q3    Short  GPT-3.5            61.81            52.16          21.85   \n",
       "21  q3    Short    GPT-4            58.79            78.74          23.73   \n",
       "15  q3    Short    Llama            48.74            61.46          23.34   \n",
       "18  q3    Short  Mistral            30.65            46.18          25.23   \n",
       "25  q3   Issues  GPT-3.5            61.40            61.39          38.88   \n",
       "22  q3   Issues    GPT-4            73.02            78.38          44.38   \n",
       "16  q3   Issues    Llama            59.53            63.71          38.88   \n",
       "19  q3   Issues  Mistral            42.79            52.51          34.45   \n",
       "13  q3   Issues    MTurk            68.18            65.93          39.86   \n",
       "\n",
       "   Accuracy CI (95\\%)  \n",
       "10       [34.0, 53.0]  \n",
       "7        [51.0, 71.0]  \n",
       "1        [16.0, 33.0]  \n",
       "4        [27.0, 46.0]  \n",
       "11       [29.0, 49.0]  \n",
       "8        [52.0, 71.0]  \n",
       "2        [11.0, 26.0]  \n",
       "5        [30.0, 49.0]  \n",
       "12       [35.0, 54.0]  \n",
       "9        [52.0, 71.0]  \n",
       "3        [16.0, 33.0]  \n",
       "6        [27.0, 46.0]  \n",
       "0        [34.0, 54.0]  \n",
       "36       [30.0, 49.0]  \n",
       "33       [25.0, 43.0]  \n",
       "27       [39.0, 59.0]  \n",
       "30       [38.0, 57.0]  \n",
       "37       [28.0, 48.0]  \n",
       "34       [18.0, 37.0]  \n",
       "28       [44.0, 63.0]  \n",
       "31       [42.0, 61.0]  \n",
       "38       [32.0, 51.0]  \n",
       "35       [33.0, 52.0]  \n",
       "29       [32.0, 50.0]  \n",
       "32       [32.0, 51.0]  \n",
       "26       [30.0, 49.0]  \n",
       "23       [20.0, 38.0]  \n",
       "20       [24.0, 43.0]  \n",
       "14       [19.0, 36.0]  \n",
       "17       [18.0, 35.0]  \n",
       "24       [14.0, 30.0]  \n",
       "21       [16.0, 32.0]  \n",
       "15       [15.0, 32.0]  \n",
       "18       [17.0, 34.0]  \n",
       "25       [29.0, 49.0]  \n",
       "22       [35.0, 54.0]  \n",
       "16       [30.0, 49.0]  \n",
       "19       [25.0, 44.0]  \n",
       "13       [29.0, 50.0]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Crowd Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the crowdsourcing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# debates_df = pd.read_json(\"../data/filtered_data/debates_filtered_df.json\")\n",
    "# debates_df[\"start_date\"] = pd.to_datetime(debates_df[\"start_date\"])\n",
    "\n",
    "# users_df = pd.read_json(\"../data/processed_data/users_df.json\")\n",
    "# users_df = users_df.reset_index(names=\"voter_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: figure out what i want to fo with these responses\n",
    "# full_df[(full_df.gpt_response.str.contains(\"Con\") & (full_df.gpt_response != \"Con\")) |\n",
    "#              (full_df.gpt_response.str.contains(\"Pro\") & (full_df.gpt_response != \"Pro\")) |\n",
    "#              (full_df.gpt_response.str.contains(\"Tie\") & (full_df.gpt_response != \"Tie\"))].groupby([\"model\", \"question\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_df = []\n",
    "# binary_class_files = glob.glob(\"../results/binary_class/*\")\n",
    "\n",
    "# for file in binary_class_files:\n",
    "#     model = file.split(\".json\")[0].split(\"/\")[-1]\n",
    "#     df = pd.read_json(file)\n",
    "#     df[\"model\"] = model\n",
    "\n",
    "#     binary_df.append(df)\n",
    "\n",
    "# binary_df = pd.concat(binary_df)\n",
    "# binary_df = process_gpt_response(binary_df)\n",
    "# binary_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# accuracies = []\n",
    "# pro_recalls = []\n",
    "# con_recalls = []\n",
    "# confidence_intervals = []\n",
    "\n",
    "# for model in list(binary_df.model.unique()):\n",
    "#     model_df = binary_df[binary_df.model == model]\n",
    "#     accuracy, recalls, precisions, bounds = get_bootstrap(model_df, \"agreed_before\")\n",
    "#     models.append(model)\n",
    "#     accuracies.append(accuracy)\n",
    "#     pro_recalls.append(recalls[0])\n",
    "#     con_recalls.append(recalls[1])\n",
    "#     confidence_intervals.append(bounds)\n",
    "\n",
    "# results = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Model\": models,\n",
    "#         \"Pro Recall (\\%)\": pro_recalls,\n",
    "#         \"Con Recall (\\%)\": con_recalls,\n",
    "#         \"Accuracy\": accuracies,\n",
    "#         \"Accuracy CI (95\\%)\": confidence_intervals,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# results = results.sort_values([\"Model\"]).reset_index(drop=True)\n",
    "# results[\"Model\"] = results.Model.str.capitalize()\n",
    "# results[\"Model\"] = results.Model.str.replace(\"Gpt\", \"GPT\")\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracies for turk\n",
    "for predict_column, truth_column in zip(\n",
    "    [\"q1\", \"q2\", \"q3\"], [\"more_convincing_arguments\", \"agreed_before\", \"agreed_after\"]\n",
    "):\n",
    "    accuracy, recalls, precisions, bounds = get_bootstrap(\n",
    "        crowd_df, truth_column, predict_column\n",
    "    )\n",
    "    questions.append(predict_column.split(\"q\")[1])\n",
    "    datasets.append(\"Turk\")\n",
    "    models.append(\"MTurk\")\n",
    "    accuracies.append(accuracy)\n",
    "    confidence_intervals.append(bounds)\n",
    "    \n",
    "    # pro_recalls.append(recalls[0])\n",
    "    # con_recalls.append(recalls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_agg = votes_df[[\"debate_id\", \"voter_id\", \"more_convincing_arguments\"]].merge(\n",
    "    ground_truth, on=\"debate_id\"\n",
    ")\n",
    "\n",
    "voter_agg_temp = voter_agg[voter_agg.debate_id.isin(PoliProp)]\n",
    "models.append(\"VoterAgg\")\n",
    "questions.append(\"1\")\n",
    "datasets.append(\"Trimmed\")\n",
    "accuracies.append(\n",
    "    (\n",
    "        voter_agg_temp.more_convincing_arguments_x\n",
    "        == voter_agg_temp.more_convincing_arguments_y\n",
    "    ).sum()\n",
    "    / len(voter_agg_temp)\n",
    "    * 100\n",
    ")\n",
    "# pro_recalls.append(\"--\")\n",
    "# con_recalls.append(\"--\")\n",
    "confidence_intervals.append(\"--\")\n",
    "\n",
    "\n",
    "voter_agg_temp = voter_agg[voter_agg.debate_id.isin(PoliPropShort)]\n",
    "models.append(\"VoterAgg\")\n",
    "questions.append(\"1\")\n",
    "datasets.append(\"Short\")\n",
    "accuracies.append(\n",
    "    (\n",
    "        voter_agg_temp.more_convincing_arguments_x\n",
    "        == voter_agg_temp.more_convincing_arguments_y\n",
    "    ).sum()\n",
    "    / len(voter_agg_temp)\n",
    "    * 100\n",
    ")\n",
    "pro_recalls.append(\"--\")\n",
    "con_recalls.append(\"--\")\n",
    "confidence_intervals.append(\"--\")\n",
    "\n",
    "voter_agg_temp = voter_agg[voter_agg.debate_id.isin(PoliPropCrowd)]\n",
    "models.append(\"VoterAgg\")\n",
    "questions.append(\"1\")\n",
    "datasets.append(\"Crowd\")\n",
    "accuracies.append(\n",
    "    (\n",
    "        voter_agg_temp.more_convincing_arguments_x\n",
    "        == voter_agg_temp.more_convincing_arguments_y\n",
    "    ).sum()\n",
    "    / len(voter_agg_temp)\n",
    "    * 100\n",
    ")\n",
    "pro_recalls.append(\"--\")\n",
    "con_recalls.append(\"--\")\n",
    "confidence_intervals.append(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"Q\": questions,\n",
    "        \"Dataset\": datasets,\n",
    "        \"Model\": models,\n",
    "        \"Accuracy\": accuracies,\n",
    "        \"Accuracy CI (95\\%)\": confidence_intervals,\n",
    "        # \"Pro Recall (\\%)\": pro_recalls,\n",
    "        # \"Con Recall (\\%)\": con_recalls,\n",
    "    }\n",
    ")\n",
    "\n",
    "results[\"Model\"] = results.Model.str.capitalize()\n",
    "results[\"Model\"] = results.Model.str.replace(\"Gpt\", \"GPT\")\n",
    "\n",
    "results[\"Dataset\"] = pd.Categorical(\n",
    "    results[\"Dataset\"], [\"Trimmed\", \"Short\", \"Crowd\", \"--\"]\n",
    ")\n",
    "results[\"Model\"] = pd.Categorical(\n",
    "    results[\"Model\"], [\"GPT-3.5\", \"GPT-4\", \"Llama\", \"Mistral\", \"Mturk\", \"Voteragg\"]\n",
    ")\n",
    "results = results.sort_values([\"Q\", \"Dataset\", \"Model\"])\n",
    "\n",
    "print(\n",
    "    results[[\"Q\", \"Dataset\", \"Model\", \"Accuracy\", \"Accuracy CI (95\\%)\"]].to_latex(\n",
    "        index=False, float_format=\"%.2f\", position=\"h\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(sns.color_palette(\"bright\"))\n",
    "fig, axs = plt.subplots(1, 3, sharey=True, sharex=True, figsize=(15, 5))\n",
    "axs = axs.flatten()\n",
    "models = [\"GPT-3.5\", \"GPT-4\", \"Llama\", \"Mistral\"]\n",
    "baselines = results[~results.Model.isin(models)]\n",
    "for i, ax in enumerate(axs):\n",
    "    sns.barplot(\n",
    "        data=results[(results.Model.isin(models)) & (results.Q == str(i + 1))],\n",
    "        x=\"Model\",\n",
    "        y=\"Accuracy\",\n",
    "        hue=\"Dataset\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    c = 3\n",
    "    for _, row in baselines[baselines.Q == str(i + 1)].iterrows():\n",
    "        if row[\"Model\"] != \"Voteragg\":\n",
    "            ax.axhline(\n",
    "                y=row[\"Accuracy\"],\n",
    "                c=sns.color_palette(\"colorblind\")[c],\n",
    "                label=row[\"Model\"],\n",
    "            )\n",
    "            c += 1\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.set_title(f\"Question {i+1}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Barplots of Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abortion Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abortion_files = glob.glob(\"../results/abortion/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(\n",
    "    df: pd.DataFrame, column: str = \"gpt_response\", reasoning=False\n",
    ") -> pd.DataFrame:\n",
    "    df = df.reset_index(names=\"vote_id\")\n",
    "    if reasoning:\n",
    "        df.rename(columns={\"gpt_response\": \"gpt_answer\"}, inplace=True)\n",
    "        df[\"gpt_response\"] = df.gpt_answer.apply(\n",
    "            lambda x: x.title().split(\"Answer: \")[-1]\n",
    "        )\n",
    "        df = df.drop(columns=\"gpt_answer\")\n",
    "    df[column] = df[column].str.replace(\".\", \"\")\n",
    "    df[column] = df[column].str.replace(\" \", \"\")\n",
    "\n",
    "    df[column] = df[column].apply(\n",
    "        lambda x: (\n",
    "            x.replace(\" \", \"\")\n",
    "            if any(vote in x for vote in [\"Pro\", \"Con\", \"Tie\"]) and len(x) <= 10\n",
    "            else \"other\"\n",
    "        )\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "datasets = []\n",
    "big_issues = []\n",
    "reasoning = []\n",
    "accuracies = []\n",
    "pro_recalls = []\n",
    "con_recalls = []\n",
    "cis = []\n",
    "\n",
    "for file in abortion_files:\n",
    "    for debates, dataset in zip(\n",
    "        [PoliPropAbortion, PoliPropCrowdAbortion], [\"Abortion\", \"CrowdAbortion\"]\n",
    "    ):\n",
    "        df = pd.read_json(file)\n",
    "        df = df[df.debate_id.isin(debates)]\n",
    "        model = file.split(\"/\")[3]\n",
    "\n",
    "        if \"BI\" in file:\n",
    "            big_issues.append(\"Yes\")\n",
    "        else:\n",
    "            big_issues.append(\"No\")\n",
    "\n",
    "        if \"R\" in file:\n",
    "            reasoning.append(\"Yes\")\n",
    "            df = process_llm_response(df, reasoning=True)\n",
    "        else:\n",
    "            reasoning.append(\"No\")\n",
    "            df = process_llm_response(df, reasoning=False)\n",
    "\n",
    "        accuracy, recalls, precisions, bounds = get_bootstrap(\n",
    "            df, \"agreed_before\", \"gpt_response\"\n",
    "        )\n",
    "\n",
    "        models.append(model)\n",
    "        datasets.append(dataset)\n",
    "        accuracies.append(accuracy)\n",
    "        pro_recalls.append(recalls[0])\n",
    "        con_recalls.append(recalls[1])\n",
    "        cis.append(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Dataset\": datasets,\n",
    "        \"Big Issues\": big_issues,\n",
    "        \"Reasoning\": reasoning,\n",
    "        \"Accuracy\": accuracies,\n",
    "        \"95 \\% CI\": cis,\n",
    "    }\n",
    ")\n",
    "\n",
    "results = results.sort_values([\"Model\", \"Big Issues\", \"Reasoning\"]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "results[\"Model\"] = results.Model.str.capitalize()\n",
    "results[\"Model\"] = results.Model.str.replace(\"Gpt\", \"GPT\")\n",
    "results = results.sort_values([\"Dataset\", \"Model\", \"Big Issues\", \"Reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, recalls, precisions, bounds = get_bootstrap(\n",
    "    crowd_df[crowd_df.debate_id.isin(PoliPropCrowdAbortion)], \"agreed_before\", \"q2\"\n",
    ")\n",
    "\n",
    "print(accuracy, bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abortion_df = pd.read_json(\"../data/processed_data/abortion_propositions.json\")\n",
    "abortion_df = abortion_df.merge(debates_df[[\"debate_id\", \"start_date\"]])\n",
    "abortion_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abortion_df = abortion_df[abortion_df.debate_id.isin(PoliPropCrowd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task_configs.json\") as f:\n",
    "    task_config = json.load(f)\n",
    "\n",
    "demographic_features = task_config[\"demographic_columns\"]\n",
    "demographic_features.remove(\"birthday\")\n",
    "big_issues_features = task_config[\"big_issue_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_stance(row, column):\n",
    "    if row[column] == \"Pro\":\n",
    "        return 1\n",
    "    elif row[column] == \"Con\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "votes_df[\"agreed_before\"] = votes_df.apply(\n",
    "    lambda x: to_stance(x, \"agreed_before\"), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_df = abortion_df[abortion_df.stance == \"Pro\"].merge(votes_df).merge(users_df)\n",
    "con_df = abortion_df[abortion_df.stance == \"Con\"].merge(votes_df).merge(users_df)\n",
    "con_df[\"agreed_before\"] = con_df.agreed_before * -1\n",
    "\n",
    "df = pd.concat([pro_df, con_df]).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"birthday\"] = pd.to_datetime(df.birthday)\n",
    "df[\"age\"] = (df.start_date - df.birthday) / pd.Timedelta(\"365 days\")\n",
    "df[\"age\"] = (df.age.max() - df.age) / (df.age.max() - df.age.min())\n",
    "df[\"age\"] = df.age.fillna(df.age.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df[[\"debate_id\", \"agreed_before\"] + demographic_features])\n",
    "df_dummies_BI = pd.get_dummies(\n",
    "    df[[\"debate_id\", \"agreed_before\"] + demographic_features + big_issues_features]\n",
    ")\n",
    "\n",
    "features = [\n",
    "    col for col in df_dummies.columns if col not in [\"debate_id\", \"agreed_before\"]\n",
    "]\n",
    "features_BI = [\n",
    "    col for col in df_dummies_BI.columns if col not in [\"debate_id\", \"agreed_before\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 20\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "\n",
    "for clf in [LR_clf, GB_clf]:\n",
    "    scores = []\n",
    "    scores_BI = []\n",
    "    for i in range(num_splits):\n",
    "        # split on the debates\n",
    "        train, test = train_test_split(list(df.debate_id.unique()))\n",
    "        # get train and test set\n",
    "        df_train = df_dummies.copy()[df_dummies.debate_id.isin(train)]\n",
    "        df_test = df_dummies.copy()[df_dummies.debate_id.isin(test)]\n",
    "\n",
    "        df_train_BI = df_dummies_BI.copy()[df_dummies_BI.debate_id.isin(train)]\n",
    "        df_test_BI = df_dummies_BI.copy()[df_dummies_BI.debate_id.isin(test)]\n",
    "\n",
    "        # get features and outputs\n",
    "        X_train = pd.get_dummies(df_train[features])\n",
    "        X_test = pd.get_dummies(df_test[features])\n",
    "        y_train = df_train[\"agreed_before\"]\n",
    "        y_test = df_test[\"agreed_before\"]\n",
    "\n",
    "        # get features and outputs\n",
    "        X_train_BI = pd.get_dummies(df_train_BI[features_BI])\n",
    "        X_test_BI = pd.get_dummies(df_test_BI[features_BI])\n",
    "        y_train_BI = df_train_BI[\"agreed_before\"]\n",
    "        y_test_BI = df_test_BI[\"agreed_before\"]\n",
    "\n",
    "        score = clf.fit(X_train, y_train).score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "        score = clf.fit(X_train_BI, y_train_BI).score(X_test_BI, y_test_BI)\n",
    "        scores_BI.append(score)\n",
    "\n",
    "    if clf == LR_clf:\n",
    "        models.append(\"Logistic Regression\")\n",
    "        models.append(\"Logistic Regression\")\n",
    "    elif clf == GB_clf:\n",
    "        models.append(\"Gradient Boosting\")\n",
    "        models.append(\"Gradient Boosting\")\n",
    "\n",
    "    sample_mean = np.mean(scores)\n",
    "    sample_std = np.std(scores, ddof=1)  # using ddof=1 for sample standard deviation\n",
    "\n",
    "    # Step 2: Determine the t-value for a 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    degrees_of_freedom = len(scores) - 1\n",
    "    t_value = t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "    # Step 3: Calculate confidence interval\n",
    "    margin_of_error = t_value * (sample_std / np.sqrt(len(scores)))\n",
    "    confidence_interval = (\n",
    "        round((sample_mean - margin_of_error) * 100, 2),\n",
    "        round((sample_mean + margin_of_error) * 100, 2),\n",
    "    )\n",
    "\n",
    "    accuracies.append(round(sample_mean * 100, 2))\n",
    "    confidence_intervals.append(confidence_interval)\n",
    "\n",
    "    sample_mean = np.mean(scores_BI)\n",
    "    sample_std = np.std(scores_BI, ddof=1)  # using ddof=1 for sample standard deviation\n",
    "\n",
    "    # Step 2: Determine the t-value for a 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    degrees_of_freedom = len(scores_BI) - 1\n",
    "    t_value = t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "    # Step 3: Calculate confidence interval\n",
    "    margin_of_error = t_value * (sample_std / np.sqrt(len(scores_BI)))\n",
    "    confidence_interval = (\n",
    "        round((sample_mean - margin_of_error) * 100, 2),\n",
    "        round((sample_mean + margin_of_error) * 100, 2),\n",
    "    )\n",
    "\n",
    "    accuracies.append(round(sample_mean * 100, 2))\n",
    "    confidence_intervals.append(confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressions = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Big Issues\": [\"No\", \"Yes\", \"No\", \"Yes\"],\n",
    "        \"Reasoning\": [\"--\"] * 4,\n",
    "        \"Accuracy\": accuracies,\n",
    "        \"95 \\% CI\": confidence_intervals,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results, regressions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[results.Dataset != \"Abortion\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_name(row):\n",
    "    name = row.Model\n",
    "    if row[\"Big Issues\"] == \"Yes\":\n",
    "        name += \"-BI\"\n",
    "    if row[\"Reasoning\"] == \"Yes\":\n",
    "        name += \"-R\"\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_BI_R(row):\n",
    "    name = \"\"\n",
    "    if row[\"Big Issues\"] == \"Yes\":\n",
    "        if name == \"\":\n",
    "            name += \"BI\"\n",
    "        else:\n",
    "            name += \"-BI\"\n",
    "    if row[\"Reasoning\"] == \"Yes\":\n",
    "        if name == \"\":\n",
    "            name += \"R\"\n",
    "        else:\n",
    "            name += \"-R\"\n",
    "    if name == \"\":\n",
    "        return \"None\"\n",
    "    return name\n",
    "\n",
    "\n",
    "results[\"full_name\"] = results.apply(lambda x: get_full_name(x), axis=1)\n",
    "results[\"BI-R\"] = results.apply(lambda x: get_BI_R(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(sns.color_palette(\"bright\"))\n",
    "\n",
    "models = [\"GPT-3.5\", \"GPT-4\", \"Llama\", \"Mistral\"]\n",
    "baselines = results[~results.Model.isin(models)]\n",
    "\n",
    "sns.barplot(\n",
    "    data=results[results.Model.isin(models)], x=\"Model\", y=\"Accuracy\", hue=\"BI-R\"\n",
    ")\n",
    "c = 4\n",
    "for _, row in baselines.iterrows():\n",
    "    plt.axhline(\n",
    "        y=row[\"Accuracy\"], label=row[\"full_name\"], c=sns.color_palette(\"bright\")[c]\n",
    "    )\n",
    "    c += 1\n",
    "plt.legend()\n",
    "plt.ylim([0, 100])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Stacked\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_q1 = full_df[(full_df.question == \"q1\")]\n",
    "full_q1 = full_q1[[\"debate_id\", \"gpt_response\", \"model\"]]\n",
    "full_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_q2 = full_df[(full_df.question == \"q2\")]\n",
    "full_q2 = full_q2[[\"debate_id\", \"voter_id\", \"gpt_response\", \"model\"]]\n",
    "full_q2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_q3 = full_df[full_df.question == \"q3\"]\n",
    "full_q3 = full_q3[[\"debate_id\", \"voter_id\", \"gpt_response\", \"model\"]]\n",
    "full_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.get_dummies(\n",
    "    full_q1.pivot(index=\"debate_id\", columns=\"model\", values=\"gpt_response\").dropna()\n",
    ").merge(ground_truth, on=\"debate_id\")\n",
    "df1[\"more_convincing_arguments\"] = df1[\"more_convincing_arguments\"].apply(\n",
    "    lambda x: 1 if x == \"Pro\" else x\n",
    ")\n",
    "df1[\"more_convincing_arguments\"] = df1[\"more_convincing_arguments\"].apply(\n",
    "    lambda x: 0 if x == \"Tie\" else x\n",
    ")\n",
    "df1[\"more_convincing_arguments\"] = df1[\"more_convincing_arguments\"].apply(\n",
    "    lambda x: -1 if x == \"Con\" else x\n",
    ")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(\n",
    "    full_q2.pivot(\n",
    "        index=[\"debate_id\", \"voter_id\"], columns=\"model\", values=\"gpt_response\"\n",
    "    )\n",
    ").merge(\n",
    "    full_df[[\"debate_id\", \"voter_id\", \"agreed_before\"]], on=[\"debate_id\", \"voter_id\"]\n",
    ")\n",
    "df2[\"agreed_before\"] = df2[\"agreed_before\"].apply(lambda x: 1 if x == \"Pro\" else x)\n",
    "df2[\"agreed_before\"] = df2[\"agreed_before\"].apply(lambda x: 0 if x == \"Tie\" else x)\n",
    "df2[\"agreed_before\"] = df2[\"agreed_before\"].apply(lambda x: -1 if x == \"Con\" else x)\n",
    "df2 = df2.drop_duplicates()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.get_dummies(\n",
    "    full_q3.pivot(\n",
    "        index=[\"debate_id\", \"voter_id\"], columns=\"model\", values=\"gpt_response\"\n",
    "    )\n",
    ").merge(\n",
    "    full_df[[\"debate_id\", \"voter_id\", \"agreed_after\"]], on=[\"debate_id\", \"voter_id\"]\n",
    ")\n",
    "df3[\"agreed_after\"] = df3[\"agreed_after\"].apply(lambda x: 1 if x == \"Pro\" else x)\n",
    "df3[\"agreed_after\"] = df3[\"agreed_after\"].apply(lambda x: 0 if x == \"Tie\" else x)\n",
    "df3[\"agreed_after\"] = df3[\"agreed_after\"].apply(lambda x: -1 if x == \"Con\" else x)\n",
    "df3 = df3.drop_duplicates()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "num_splits = 20\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "for clf in [LR_clf, GB_clf]:\n",
    "    scores = []\n",
    "    for i in range(num_splits):\n",
    "        # split on the debates\n",
    "        train, test = train_test_split(list(df1.debate_id.unique()))\n",
    "\n",
    "        # get train and test set\n",
    "        df1_train = df1.copy()[df1.debate_id.isin(train)]\n",
    "        df1_test = df1.copy()[df1.debate_id.isin(test)]\n",
    "\n",
    "        # get features and outputs\n",
    "        X_train = df1_train.drop(columns=[\"debate_id\", \"more_convincing_arguments\"])\n",
    "        X_test = df1_test.drop(columns=[\"debate_id\", \"more_convincing_arguments\"])\n",
    "        y_train = df1_train[\"more_convincing_arguments\"]\n",
    "        y_test = df1_test[\"more_convincing_arguments\"]\n",
    "\n",
    "        score = GB_clf.fit(X_train, y_train).score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Step 1: Calculate sample mean and sample standard deviation\n",
    "    sample_mean = np.mean(scores)\n",
    "    sample_std = np.std(scores, ddof=1)  # using ddof=1 for sample standard deviation\n",
    "\n",
    "    # Step 2: Determine the t-value for a 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    degrees_of_freedom = len(scores) - 1\n",
    "    t_value = t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "    # Step 3: Calculate confidence interval\n",
    "    margin_of_error = t_value * (sample_std / np.sqrt(len(scores)))\n",
    "    confidence_interval = (\n",
    "        round((sample_mean - margin_of_error) * 100, 2),\n",
    "        round((sample_mean + margin_of_error) * 100, 2),\n",
    "    )\n",
    "\n",
    "    print(\"Sample Mean:\", round(sample_mean * 100, 2))\n",
    "    print(\"Confidence Interval (95%):\", confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2, Q3\n",
    "\n",
    "num_splits = 20\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "for df, output in zip([df2, df3], [\"agreed_before\", \"agreed_after\"]):\n",
    "    for clf in [LR_clf, GB_clf]:\n",
    "        scores = []\n",
    "        for i in range(num_splits):\n",
    "            # split on the debates\n",
    "            train, test = train_test_split(list(df.debate_id.unique()))\n",
    "\n",
    "            # get train and test set\n",
    "            df_train = df.copy()[df.debate_id.isin(train)]\n",
    "            df_test = df.copy()[df.debate_id.isin(test)]\n",
    "\n",
    "            # get features and outputs\n",
    "            X_train = df_train.drop(columns=[\"debate_id\", \"voter_id\", output])\n",
    "            X_test = df_test.drop(columns=[\"debate_id\", \"voter_id\", output])\n",
    "            y_train = df_train[output]\n",
    "            y_test = df_test[output]\n",
    "\n",
    "            score = GB_clf.fit(X_train, y_train).score(X_test, y_test)\n",
    "            scores.append(score)\n",
    "\n",
    "        sample_mean = np.mean(scores)\n",
    "        sample_std = np.std(\n",
    "            scores, ddof=1\n",
    "        )  # using ddof=1 for sample standard deviation\n",
    "\n",
    "        # Step 2: Determine the t-value for a 95% confidence interval\n",
    "        confidence_level = 0.95\n",
    "        degrees_of_freedom = len(scores) - 1\n",
    "        t_value = t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "        # Step 3: Calculate confidence interval\n",
    "        margin_of_error = t_value * (sample_std / np.sqrt(len(scores)))\n",
    "        confidence_interval = (\n",
    "            round((sample_mean - margin_of_error) * 100, 2),\n",
    "            round((sample_mean + margin_of_error) * 100, 2),\n",
    "        )\n",
    "\n",
    "        print(\"Sample Mean:\", round(sample_mean * 100, 2))\n",
    "        print(\"Confidence Interval (95%):\", confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
