{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "from helpers.metrics import get_bootstrap\n",
    "from helpers.process_results import (\n",
    "    majority_vote,\n",
    "    process_crowdsourcing_data,\n",
    "    process_gpt_response,\n",
    ")\n",
    "from scipy.stats import t\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Basic Data Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and get statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ground truth dataframe for Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_id</th>\n",
       "      <th>q1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>Tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   debate_id   q1\n",
       "0          0  Con\n",
       "1          1  Con\n",
       "2          3  Pro\n",
       "3          4  Con\n",
       "4          5  Con\n",
       "5          7  Con\n",
       "6          8  Pro\n",
       "7          9  Tie\n",
       "8         10  Pro\n",
       "9         11  Con"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_df = pd.read_json(\"../data/processed_data/votes_df.json\")\n",
    "\n",
    "ground_truth_df = votes_df[[\"debate_id\", \"more_convincing_arguments\"]]\n",
    "ground_truth_df = ground_truth_df.rename(columns={\"more_convincing_arguments\": \"q1\"})\n",
    "ground_truth_df = (\n",
    "    ground_truth_df.groupby(\"debate_id\")\n",
    "    .apply(lambda x: majority_vote(x, \"q1\"))\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"q1\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ground_truth_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all LLM data in one DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>debate_length</th>\n",
       "      <th>model</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>voter_id</th>\n",
       "      <th>response</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>correct_form</th>\n",
       "      <th>answer_extracted</th>\n",
       "      <th>q1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>full</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q2</td>\n",
       "      <td>full</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>358</td>\n",
       "      <td>imabench</td>\n",
       "      <td>Con</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q2</td>\n",
       "      <td>full</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>358</td>\n",
       "      <td>9spaceking</td>\n",
       "      <td>Con</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q3</td>\n",
       "      <td>full</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>358</td>\n",
       "      <td>imabench</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q3</td>\n",
       "      <td>full</td>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>358</td>\n",
       "      <td>9spaceking</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question debate_length    model  debate_id    voter_id response   q2   q3  \\\n",
       "0       q1          full  GPT-3.5        358         NaN      Pro  NaN  NaN   \n",
       "1       q2          full  GPT-3.5        358    imabench      Con  Tie  Tie   \n",
       "2       q2          full  GPT-3.5        358  9spaceking      Con  Tie  Tie   \n",
       "3       q3          full  GPT-3.5        358    imabench      Pro  Tie  Tie   \n",
       "4       q3          full  GPT-3.5        358  9spaceking      Pro  Tie  Tie   \n",
       "\n",
       "   correct_form  answer_extracted   q1  \n",
       "0          True              True  Con  \n",
       "1          True              True  Con  \n",
       "2          True              True  Con  \n",
       "3          True              True  Con  \n",
       "4          True              True  Con  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propositions_df = pd.read_json(\"../data/raw_data/propositions.json\")\n",
    "PoliPropDataset = list(propositions_df.debate_id.unique())\n",
    "\n",
    "full_list = []\n",
    "all_results_files = glob.glob(\"../results/*/q*/*q*\")\n",
    "\n",
    "for file in all_results_files:\n",
    "    model = file.split(\"/\")[2]\n",
    "    question = file.split(\"/\")[3]\n",
    "\n",
    "    df = pd.read_json(file)\n",
    "    df[\"model\"] = model\n",
    "    df[\"question\"] = question\n",
    "\n",
    "    full_list.append(df)\n",
    "\n",
    "full_df = pd.concat(full_list)\n",
    "full_df = full_df[full_df.debate_id.isin(PoliPropDataset)]\n",
    "full_df = process_gpt_response(full_df)\n",
    "\n",
    "full_df = full_df[\n",
    "    [\n",
    "        \"question\",\n",
    "        \"debate_length\",\n",
    "        \"model\",\n",
    "        \"debate_id\",\n",
    "        \"voter_id\",\n",
    "        \"gpt_response\",\n",
    "        \"agreed_before\",\n",
    "        \"agreed_after\",\n",
    "        \"correct_form\",\n",
    "        \"answer_extracted\",\n",
    "    ]\n",
    "]\n",
    "full_df = full_df.rename(\n",
    "    columns={\n",
    "        \"gpt_response\": \"response\",\n",
    "        \"agreed_before\": \"q2\",\n",
    "        \"agreed_after\": \"q3\",\n",
    "    }\n",
    ")\n",
    "full_df = full_df.merge(ground_truth_df, on=\"debate_id\")\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring all the models and questions have the same debate ids and voter ids\n",
    "# display(full_df.groupby([\"question\", \"model\"]).debate_id.nunique()) # UNCOMMENT to view\n",
    "# display(full_df.groupby([\"question\", \"model\"]).voter_id.nunique()) # UNCOMMENT line to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_id</th>\n",
       "      <th>proposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>358</td>\n",
       "      <td>The September 11Th Attacks Were Orchestrated B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>376</td>\n",
       "      <td>The September 11Th Attacks Were Orchestrated B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>384</td>\n",
       "      <td>The Buildings Collapsed On September 11Th Nece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551</td>\n",
       "      <td>Poverty Can Be Eliminated By Creating A Privat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>553</td>\n",
       "      <td>In The Case Of District Of Columbia V. Heller,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>76836</td>\n",
       "      <td>Voluntary Abortion Should Be Legal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>76983</td>\n",
       "      <td>Women Make Better Politicians Than Men.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>76991</td>\n",
       "      <td>Women Should Be Allowed To Serve On U.S. Subma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>77285</td>\n",
       "      <td>Having A \"Green\" Infrastructure Would Help Ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>77647</td>\n",
       "      <td>The Confederate Flag Is Racist.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>852 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      debate_id                                        proposition\n",
       "0           358  The September 11Th Attacks Were Orchestrated B...\n",
       "1           376  The September 11Th Attacks Were Orchestrated B...\n",
       "2           384  The Buildings Collapsed On September 11Th Nece...\n",
       "3           551  Poverty Can Be Eliminated By Creating A Privat...\n",
       "4           553  In The Case Of District Of Columbia V. Heller,...\n",
       "...         ...                                                ...\n",
       "1042      76836                Voluntary Abortion Should Be Legal.\n",
       "1043      76983            Women Make Better Politicians Than Men.\n",
       "1044      76991  Women Should Be Allowed To Serve On U.S. Subma...\n",
       "1047      77285  Having A \"Green\" Infrastructure Would Help Ame...\n",
       "1049      77647                    The Confederate Flag Is Racist.\n",
       "\n",
       "[852 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propositions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In The Case Of District Of Columbia V. Heller, Heller'S Argument Is Consistent With The Constituion.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propositions_df[propositions_df.debate_id == 553].proposition.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_debates = (\n",
    "    full_df[full_df.debate_length == \"full\"]\n",
    "    .groupby([\"question\", \"model\"])\n",
    "    .debate_id.unique()\n",
    ")\n",
    "short_current_set = set(short_debates[0])\n",
    "for sds in short_debates:\n",
    "    short_current_set = short_current_set.intersection(set(sds))\n",
    "\n",
    "\n",
    "trimmed_debates = full_df.groupby([\"question\", \"model\"]).debate_id.unique()\n",
    "trimmed_current_set = set(trimmed_debates[0])\n",
    "for tds in trimmed_debates:\n",
    "    trimmed_current_set = trimmed_current_set.intersection(set(tds))\n",
    "\n",
    "SHORT = list(short_current_set)\n",
    "TRIMMED = list(trimmed_current_set)\n",
    "ABORTION = list(\n",
    "    propositions_df[\n",
    "        (propositions_df.proposition.str.lower().str.contains(\"abortion\"))\n",
    "    ].debate_id.unique()\n",
    ")\n",
    "GAY_MARRIAGE = list(\n",
    "    propositions_df[\n",
    "        (\n",
    "            propositions_df.proposition.str.lower().str.contains(\"same sex\")\n",
    "            | propositions_df.proposition.str.lower().str.contains(\"gay\")\n",
    "            | propositions_df.proposition.str.lower().str.contains(\"same-sex\")\n",
    "        )\n",
    "        & (propositions_df.proposition.str.lower().str.contains(\"marriage\"))\n",
    "    ].debate_id.unique()\n",
    ")\n",
    "\n",
    "CAPITAL_PUNISHMENT = list(\n",
    "    propositions_df[\n",
    "        (\n",
    "            propositions_df.proposition.str.lower().str.contains(\"death penalty\")\n",
    "            | propositions_df.proposition.str.lower().str.contains(\"capital punishment\")\n",
    "        )\n",
    "    ].debate_id.unique()\n",
    ")\n",
    "\n",
    "ISSUES = list(set(ABORTION + GAY_MARRIAGE + CAPITAL_PUNISHMENT))\n",
    "\n",
    "DATASETS = [TRIMMED, SHORT, ISSUES]\n",
    "DATASET_NAMES = [\"Trimmed\", \"Short\", \"Issues\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Number of debates</th>\n",
       "      <th>Number of votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trimmed</td>\n",
       "      <td>833</td>\n",
       "      <td>4871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Short</td>\n",
       "      <td>276</td>\n",
       "      <td>1538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Issues</td>\n",
       "      <td>127</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset  Number of debates  Number of votes\n",
       "0  Trimmed                833             4871\n",
       "1    Short                276             1538\n",
       "2   Issues                127              836"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_filtered_df = pd.read_json(\"../data/filtered_data/votes_filtered_df.json\")\n",
    "\n",
    "num_debates = []\n",
    "num_votes = []\n",
    "for debate_ids in [TRIMMED, SHORT, ISSUES]:\n",
    "    num_debates.append(len(debate_ids))\n",
    "    num_votes.append(\n",
    "        (len(votes_filtered_df[votes_filtered_df.debate_id.isin(debate_ids)]))\n",
    "    )\n",
    "\n",
    "datasets_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Dataset\": DATASET_NAMES,\n",
    "        \"Number of debates\": num_debates,\n",
    "        \"Number of votes\": num_votes,\n",
    "    }\n",
    ")\n",
    "datasets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare MTurk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_id</th>\n",
       "      <th>voter_id</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   debate_id voter_id question response   q1    q2    q3\n",
       "0        706      NaN       q1      Con  Pro  None  None\n",
       "1        706      NaN       q1      Con  Pro  None  None\n",
       "2        706      NaN       q1      Pro  Pro  None  None\n",
       "3        706      NaN       q1      Con  Pro  None  None\n",
       "4        706      NaN       q1      Con  Pro  None  None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all crowdsourcing files\n",
    "crowd_files = glob.glob(\"../data/raw_data/crowd/*\")\n",
    "crowd_dfs = []\n",
    "for file in crowd_files:\n",
    "    crowd_df = pd.read_csv(file)\n",
    "    crowd_dfs.append(crowd_df)\n",
    "\n",
    "# create one dataframe of all crowdsourcing data\n",
    "crowd_df = pd.concat(crowd_dfs).reset_index(drop=True)\n",
    "crowd_df = process_crowdsourcing_data(crowd_df)\n",
    "crowd_df = crowd_df.groupby([\"debate_id\", \"voter_id\"]).sample(1)\n",
    "crowd_df = pd.melt(\n",
    "    crowd_df,\n",
    "    id_vars=[\"debate_id\", \"voter_id\"],\n",
    "    value_vars=[\"q1\", \"q2\", \"q3\"],\n",
    "    var_name=\"question\",\n",
    "    value_name=\"response\",\n",
    ")\n",
    "\n",
    "crowd_df[\"voter_id\"] = crowd_df.apply(\n",
    "    lambda x: np.nan if x.question == \"q1\" else x.voter_id, axis=1\n",
    ")\n",
    "crowd_df = crowd_df.merge(\n",
    "    full_df.groupby([\"question\", \"debate_id\", \"voter_id\"], dropna=False)\n",
    "    .first()\n",
    "    .reset_index()[[\"question\", \"debate_id\", \"voter_id\", \"q1\", \"q2\", \"q3\"]],\n",
    "    on=[\"question\", \"debate_id\", \"voter_id\"],\n",
    ")\n",
    "\n",
    "crowd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get table for correct form and answer extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\begin{tabular}{llrr}\n",
      "\\toprule\n",
      "question & model & correct_form & answer_extracted \\\\\n",
      "\\midrule\n",
      "q1 & GPT-3.5 & 99.88 & 100.00 \\\\\n",
      "q1 & GPT-4 & 99.06 & 100.00 \\\\\n",
      "q1 & Llama & 0.00 & 94.95 \\\\\n",
      "q1 & Mistral & 62.79 & 95.07 \\\\\n",
      "q2 & GPT-3.5 & 99.84 & 99.88 \\\\\n",
      "q2 & GPT-4 & 100.00 & 100.00 \\\\\n",
      "q2 & Llama & 0.00 & 97.13 \\\\\n",
      "q2 & Mistral & 67.13 & 100.00 \\\\\n",
      "q3 & GPT-3.5 & 99.82 & 99.94 \\\\\n",
      "q3 & GPT-4 & 99.61 & 99.98 \\\\\n",
      "q3 & Llama & 0.00 & 91.50 \\\\\n",
      "q3 & Mistral & 17.22 & 79.72 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_form_df = (\n",
    "    full_df.groupby([\"question\", \"model\"])[[\"correct_form\", \"answer_extracted\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "correct_form_df[[\"correct_form\", \"answer_extracted\"]] = (\n",
    "    correct_form_df[[\"correct_form\", \"answer_extracted\"]] * 100\n",
    ")\n",
    "\n",
    "print(correct_form_df.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the accuracies for each configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "datasets = []\n",
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "pro_recalls = []\n",
    "con_recalls = []\n",
    "\n",
    "for question in full_df.question.unique():\n",
    "    accuracy, recalls, _, ci = get_bootstrap(\n",
    "        crowd_df[(crowd_df.debate_id.isin(ISSUES)) & (crowd_df.question == question)],\n",
    "        question,\n",
    "    )\n",
    "\n",
    "    questions.append(question)\n",
    "    datasets.append(\"Issues\")\n",
    "    models.append(\"MTurk\")\n",
    "    accuracies.append(accuracy)\n",
    "    confidence_intervals.append(ci)\n",
    "    pro_recalls.append(recalls[0])\n",
    "    con_recalls.append(recalls[1])\n",
    "\n",
    "    for model in full_df.model.unique():\n",
    "        for dataset, name in zip(\n",
    "            [TRIMMED, SHORT, ISSUES], [\"Trimmed\", \"Short\", \"Issues\"]\n",
    "        ):\n",
    "\n",
    "            temp_df = full_df[\n",
    "                (full_df.question == question)\n",
    "                & (full_df.model == model)\n",
    "                & (full_df.debate_id.isin(dataset))\n",
    "            ]\n",
    "            accuracy, recalls, _, ci = get_bootstrap(temp_df, question)\n",
    "\n",
    "            questions.append(question)\n",
    "            datasets.append(name)\n",
    "            models.append(model)\n",
    "            accuracies.append(accuracy)\n",
    "            confidence_intervals.append(ci)\n",
    "            pro_recalls.append(recalls[0])\n",
    "            con_recalls.append(recalls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\begin{tabular}{lllrrrl}\n",
      "\\toprule\n",
      "Q & Dataset & Model & Pro Recall (\\%) & Con Recall (\\%) & Accuracy (\\%) & Accuracy CI (95\\%) \\\\\n",
      "\\midrule\n",
      "q1 & Trimmed & GPT-3.5 & 68.77 & 30.73 & 42.74 & (33.0, 52.0) \\\\\n",
      "q1 & Trimmed & GPT-4 & 54.89 & 77.07 & 60.50 & (51.0, 70.0) \\\\\n",
      "q1 & Trimmed & Llama & 49.84 & 0.24 & 23.65 & (15.0, 32.0) \\\\\n",
      "q1 & Trimmed & Mistral & 74.13 & 13.00 & 36.01 & (27.0, 46.0) \\\\\n",
      "q1 & Short & GPT-3.5 & 64.29 & 27.70 & 39.13 & (30.0, 48.0) \\\\\n",
      "q1 & Short & GPT-4 & 57.14 & 77.03 & 61.96 & (53.0, 72.0) \\\\\n",
      "q1 & Short & Llama & 35.71 & 0.00 & 17.75 & (11.0, 27.0) \\\\\n",
      "q1 & Short & Mistral & 73.47 & 20.27 & 39.13 & (29.0, 49.0) \\\\\n",
      "q1 & Issues & GPT-3.5 & 69.09 & 29.03 & 44.09 & (35.0, 54.0) \\\\\n",
      "q1 & Issues & GPT-4 & 69.09 & 66.13 & 62.20 & (53.0, 72.0) \\\\\n",
      "q1 & Issues & Llama & 47.27 & 0.00 & 24.41 & (16.0, 33.0) \\\\\n",
      "q1 & Issues & Mistral & 70.91 & 9.68 & 37.01 & (28.0, 46.0) \\\\\n",
      "q1 & Issues & MTurk & 56.34 & 39.69 & 44.19 & (34.0, 54.0) \\\\\n",
      "q2 & Trimmed & GPT-3.5 & 28.51 & 47.20 & 39.36 & (30.0, 49.0) \\\\\n",
      "q2 & Trimmed & GPT-4 & 58.91 & 66.28 & 34.18 & (26.0, 44.0) \\\\\n",
      "q2 & Trimmed & Llama & 24.63 & 0.62 & 49.25 & (39.0, 59.0) \\\\\n",
      "q2 & Trimmed & Mistral & 23.69 & 13.58 & 46.97 & (37.0, 56.0) \\\\\n",
      "q2 & Short & GPT-3.5 & 25.25 & 43.53 & 37.52 & (28.0, 47.0) \\\\\n",
      "q2 & Short & GPT-4 & 54.46 & 70.50 & 26.79 & (18.0, 36.0) \\\\\n",
      "q2 & Short & Llama & 22.28 & 1.08 & 54.42 & (45.0, 64.0) \\\\\n",
      "q2 & Short & Mistral & 23.27 & 13.31 & 51.56 & (42.0, 62.0) \\\\\n",
      "q2 & Issues & GPT-3.5 & 28.37 & 67.07 & 41.87 & (32.0, 52.0) \\\\\n",
      "q2 & Issues & GPT-4 & 67.79 & 73.17 & 42.82 & (33.0, 53.0) \\\\\n",
      "q2 & Issues & Llama & 35.10 & 0.81 & 41.27 & (32.0, 51.0) \\\\\n",
      "q2 & Issues & Mistral & 21.15 & 15.04 & 41.39 & (32.0, 52.0) \\\\\n",
      "q2 & Issues & MTurk & 63.53 & 66.82 & 39.32 & (30.0, 49.0) \\\\\n",
      "q3 & Trimmed & GPT-3.5 & 58.68 & 51.08 & 29.11 & (20.0, 38.0) \\\\\n",
      "q3 & Trimmed & GPT-4 & 62.09 & 78.41 & 33.48 & (25.0, 43.0) \\\\\n",
      "q3 & Trimmed & Llama & 41.42 & 59.30 & 27.49 & (19.0, 36.0) \\\\\n",
      "q3 & Trimmed & Mistral & 33.50 & 44.68 & 26.07 & (18.0, 35.0) \\\\\n",
      "q3 & Short & GPT-3.5 & 61.81 & 52.16 & 21.85 & (14.0, 30.0) \\\\\n",
      "q3 & Short & GPT-4 & 58.79 & 78.74 & 23.73 & (16.0, 33.0) \\\\\n",
      "q3 & Short & Llama & 48.74 & 61.46 & 23.34 & (15.0, 31.0) \\\\\n",
      "q3 & Short & Mistral & 30.65 & 46.18 & 25.23 & (17.0, 34.0) \\\\\n",
      "q3 & Issues & GPT-3.5 & 61.40 & 61.39 & 38.88 & (30.0, 49.0) \\\\\n",
      "q3 & Issues & GPT-4 & 73.02 & 78.38 & 44.38 & (35.0, 54.0) \\\\\n",
      "q3 & Issues & Llama & 59.53 & 63.71 & 38.88 & (29.0, 48.0) \\\\\n",
      "q3 & Issues & Mistral & 42.79 & 52.51 & 34.45 & (26.0, 44.0) \\\\\n",
      "q3 & Issues & MTurk & 68.18 & 65.93 & 39.86 & (31.0, 50.0) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"Q\": questions,\n",
    "        \"Dataset\": datasets,\n",
    "        \"Model\": models,\n",
    "        \"Pro Recall (\\%)\": pro_recalls,\n",
    "        \"Con Recall (\\%)\": con_recalls,\n",
    "        \"Accuracy (\\%)\": accuracies,\n",
    "        \"Accuracy CI (95\\%)\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "results[\"Dataset\"] = pd.Categorical(results[\"Dataset\"], [\"Trimmed\", \"Short\", \"Issues\"])\n",
    "results[\"Model\"] = pd.Categorical(\n",
    "    results[\"Model\"], [\"GPT-3.5\", \"GPT-4\", \"Llama\", \"Mistral\", \"MTurk\"]\n",
    ")\n",
    "results = results.sort_values([\"Q\", \"Dataset\", \"Model\"])\n",
    "\n",
    "print(results.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Crowd Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the crowdsourcing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debates_df = pd.read_json(\"../data/filtered_data/debates_filtered_df.json\")\n",
    "# debates_df[\"start_date\"] = pd.to_datetime(debates_df[\"start_date\"])\n",
    "\n",
    "# users_df = pd.read_json(\"../data/processed_data/users_df.json\")\n",
    "# users_df = users_df.reset_index(names=\"voter_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: figure out what i want to fo with these responses\n",
    "# full_df[(full_df.gpt_response.str.contains(\"Con\") & (full_df.gpt_response != \"Con\")) |\n",
    "#              (full_df.gpt_response.str.contains(\"Pro\") & (full_df.gpt_response != \"Pro\")) |\n",
    "#              (full_df.gpt_response.str.contains(\"Tie\") & (full_df.gpt_response != \"Tie\"))].groupby([\"model\", \"question\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_df = []\n",
    "# binary_class_files = glob.glob(\"../results/binary_class/*\")\n",
    "\n",
    "# for file in binary_class_files:\n",
    "#     model = file.split(\".json\")[0].split(\"/\")[-1]\n",
    "#     df = pd.read_json(file)\n",
    "#     df[\"model\"] = model\n",
    "\n",
    "#     binary_df.append(df)\n",
    "\n",
    "# binary_df = pd.concat(binary_df)\n",
    "# binary_df = process_gpt_response(binary_df)\n",
    "# binary_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# accuracies = []\n",
    "# pro_recalls = []\n",
    "# con_recalls = []\n",
    "# confidence_intervals = []\n",
    "\n",
    "# for model in list(binary_df.model.unique()):\n",
    "#     model_df = binary_df[binary_df.model == model]\n",
    "#     accuracy, recalls, precisions, bounds = get_bootstrap(model_df, \"agreed_before\")\n",
    "#     models.append(model)\n",
    "#     accuracies.append(accuracy)\n",
    "#     pro_recalls.append(recalls[0])\n",
    "#     con_recalls.append(recalls[1])\n",
    "#     confidence_intervals.append(bounds)\n",
    "\n",
    "# results = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Model\": models,\n",
    "#         \"Pro Recall (\\%)\": pro_recalls,\n",
    "#         \"Con Recall (\\%)\": con_recalls,\n",
    "#         \"Accuracy\": accuracies,\n",
    "#         \"Accuracy CI (95\\%)\": confidence_intervals,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# results = results.sort_values([\"Model\"]).reset_index(drop=True)\n",
    "# results[\"Model\"] = results.Model.str.capitalize()\n",
    "# results[\"Model\"] = results.Model.str.replace(\"Gpt\", \"GPT\")\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_agg = votes_df[[\"debate_id\", \"voter_id\", \"more_convincing_arguments\"]].merge(\n",
    "    ground_truth, on=\"debate_id\"\n",
    ")\n",
    "\n",
    "voter_agg_temp = voter_agg[voter_agg.debate_id.isin(PoliProp)]\n",
    "models.append(\"VoterAgg\")\n",
    "questions.append(\"1\")\n",
    "datasets.append(\"Trimmed\")\n",
    "accuracies.append(\n",
    "    (\n",
    "        voter_agg_temp.more_convincing_arguments_x\n",
    "        == voter_agg_temp.more_convincing_arguments_y\n",
    "    ).sum()\n",
    "    / len(voter_agg_temp)\n",
    "    * 100\n",
    ")\n",
    "# pro_recalls.append(\"--\")\n",
    "# con_recalls.append(\"--\")\n",
    "confidence_intervals.append(\"--\")\n",
    "\n",
    "\n",
    "voter_agg_temp = voter_agg[voter_agg.debate_id.isin(PoliPropShort)]\n",
    "models.append(\"VoterAgg\")\n",
    "questions.append(\"1\")\n",
    "datasets.append(\"Short\")\n",
    "accuracies.append(\n",
    "    (\n",
    "        voter_agg_temp.more_convincing_arguments_x\n",
    "        == voter_agg_temp.more_convincing_arguments_y\n",
    "    ).sum()\n",
    "    / len(voter_agg_temp)\n",
    "    * 100\n",
    ")\n",
    "pro_recalls.append(\"--\")\n",
    "con_recalls.append(\"--\")\n",
    "confidence_intervals.append(\"--\")\n",
    "\n",
    "voter_agg_temp = voter_agg[voter_agg.debate_id.isin(PoliPropCrowd)]\n",
    "models.append(\"VoterAgg\")\n",
    "questions.append(\"1\")\n",
    "datasets.append(\"Crowd\")\n",
    "accuracies.append(\n",
    "    (\n",
    "        voter_agg_temp.more_convincing_arguments_x\n",
    "        == voter_agg_temp.more_convincing_arguments_y\n",
    "    ).sum()\n",
    "    / len(voter_agg_temp)\n",
    "    * 100\n",
    ")\n",
    "pro_recalls.append(\"--\")\n",
    "con_recalls.append(\"--\")\n",
    "confidence_intervals.append(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(sns.color_palette(\"bright\"))\n",
    "fig, axs = plt.subplots(1, 3, sharey=True, sharex=True, figsize=(15, 5))\n",
    "axs = axs.flatten()\n",
    "models = [\"GPT-3.5\", \"GPT-4\", \"Llama\", \"Mistral\"]\n",
    "baselines = results[~results.Model.isin(models)]\n",
    "for i, ax in enumerate(axs):\n",
    "    sns.barplot(\n",
    "        data=results[(results.Model.isin(models)) & (results.Q == str(i + 1))],\n",
    "        x=\"Model\",\n",
    "        y=\"Accuracy\",\n",
    "        hue=\"Dataset\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    c = 3\n",
    "    for _, row in baselines[baselines.Q == str(i + 1)].iterrows():\n",
    "        if row[\"Model\"] != \"Voteragg\":\n",
    "            ax.axhline(\n",
    "                y=row[\"Accuracy\"],\n",
    "                c=sns.color_palette(\"colorblind\")[c],\n",
    "                label=row[\"Model\"],\n",
    "            )\n",
    "            c += 1\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.set_title(f\"Question {i+1}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Barplots of Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abortion Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abortion_files = glob.glob(\"../results/abortion/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_id</th>\n",
       "      <th>voter_id</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>76836</td>\n",
       "      <td>Ragnar</td>\n",
       "      <td>q3</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>76836</td>\n",
       "      <td>Splenic_Warrior</td>\n",
       "      <td>q3</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>76836</td>\n",
       "      <td>birdlandmemories</td>\n",
       "      <td>q3</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Con</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>76836</td>\n",
       "      <td>debatability</td>\n",
       "      <td>q3</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Tie</td>\n",
       "      <td>Tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>76836</td>\n",
       "      <td>kinsky</td>\n",
       "      <td>q3</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2254 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      debate_id          voter_id question response   q1    q2    q3\n",
       "0           706               NaN       q1      Con  Pro  None  None\n",
       "1           706               NaN       q1      Con  Pro  None  None\n",
       "2           706               NaN       q1      Pro  Pro  None  None\n",
       "3           706               NaN       q1      Con  Pro  None  None\n",
       "4           706               NaN       q1      Con  Pro  None  None\n",
       "...         ...               ...      ...      ...  ...   ...   ...\n",
       "2249      76836            Ragnar       q3      Pro  Pro   Pro   Pro\n",
       "2250      76836   Splenic_Warrior       q3      Pro  Pro   Tie   Tie\n",
       "2251      76836  birdlandmemories       q3      Pro  Pro   Con   Con\n",
       "2252      76836      debatability       q3      Pro  Pro   Tie   Tie\n",
       "2253      76836            kinsky       q3      Pro  Pro   Pro   Pro\n",
       "\n",
       "[2254 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(\n",
    "    df: pd.DataFrame, column: str = \"gpt_response\", reasoning=False\n",
    ") -> pd.DataFrame:\n",
    "    df = df.reset_index(names=\"vote_id\")\n",
    "    if reasoning:\n",
    "        df.rename(columns={\"gpt_response\": \"gpt_answer\"}, inplace=True)\n",
    "        df[\"gpt_response\"] = df.gpt_answer.apply(\n",
    "            lambda x: x.title().split(\"Answer: \")[-1]\n",
    "        )\n",
    "        df = df.drop(columns=\"gpt_answer\")\n",
    "    df[column] = df[column].str.replace(\".\", \"\")\n",
    "    df[column] = df[column].str.replace(\" \", \"\")\n",
    "\n",
    "    df[column] = df[column].apply(\n",
    "        lambda x: (\n",
    "            x.replace(\" \", \"\")\n",
    "            if any(vote in x for vote in [\"Pro\", \"Con\", \"Tie\"]) and len(x) <= 10\n",
    "            else \"other\"\n",
    "        )\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "datasets = []\n",
    "big_issues = []\n",
    "reasoning = []\n",
    "accuracies = []\n",
    "pro_recalls = []\n",
    "con_recalls = []\n",
    "cis = []\n",
    "\n",
    "for file in abortion_files:\n",
    "    for debates, dataset in zip(\n",
    "        [PoliPropAbortion, PoliPropCrowdAbortion], [\"Abortion\", \"CrowdAbortion\"]\n",
    "    ):\n",
    "        df = pd.read_json(file)\n",
    "        df = df[df.debate_id.isin(debates)]\n",
    "        model = file.split(\"/\")[3]\n",
    "\n",
    "        if \"BI\" in file:\n",
    "            big_issues.append(\"Yes\")\n",
    "        else:\n",
    "            big_issues.append(\"No\")\n",
    "\n",
    "        if \"R\" in file:\n",
    "            reasoning.append(\"Yes\")\n",
    "            df = process_llm_response(df, reasoning=True)\n",
    "        else:\n",
    "            reasoning.append(\"No\")\n",
    "            df = process_llm_response(df, reasoning=False)\n",
    "\n",
    "        accuracy, recalls, precisions, bounds = get_bootstrap(\n",
    "            df, \"agreed_before\", \"gpt_response\"\n",
    "        )\n",
    "\n",
    "        models.append(model)\n",
    "        datasets.append(dataset)\n",
    "        accuracies.append(accuracy)\n",
    "        pro_recalls.append(recalls[0])\n",
    "        con_recalls.append(recalls[1])\n",
    "        cis.append(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Dataset\": datasets,\n",
    "        \"Big Issues\": big_issues,\n",
    "        \"Reasoning\": reasoning,\n",
    "        \"Accuracy\": accuracies,\n",
    "        \"95 \\% CI\": cis,\n",
    "    }\n",
    ")\n",
    "\n",
    "results = results.sort_values([\"Model\", \"Big Issues\", \"Reasoning\"]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "results[\"Model\"] = results.Model.str.capitalize()\n",
    "results[\"Model\"] = results.Model.str.replace(\"Gpt\", \"GPT\")\n",
    "results = results.sort_values([\"Dataset\", \"Model\", \"Big Issues\", \"Reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some things needed for regression\n",
    "def to_stance(row, column):\n",
    "    if row[column] == \"Pro\":\n",
    "        return 1\n",
    "    elif row[column] == \"Con\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "with open(\"task_configs.json\") as f:\n",
    "    task_config = json.load(f)\n",
    "\n",
    "demographic_features = task_config[\"demographic_columns\"]\n",
    "demographic_features.remove(\"birthday\")\n",
    "big_issues_features = task_config[\"big_issue_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_issue = \"Abortion\"\n",
    "\n",
    "if current_issue == \"Gay Marriage\":\n",
    "    debates = GAY_MARRIAGE\n",
    "    path_to_stances = \"../data/processed_data/gay_marriage_props.json\"\n",
    "elif current_issue == \"Abortion\":\n",
    "    debates = ABORTION\n",
    "    path_to_stances = \"../data/processed_data/abortion_props.json\"\n",
    "elif current_issue == \"Capital Punishment\":\n",
    "    debates = CAPITAL_PUNISHMENT\n",
    "    path_to_stances = \"../data/processed_data/capital_punishment_props.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_id</th>\n",
       "      <th>voter_id</th>\n",
       "      <th>agreed_before</th>\n",
       "      <th>birthday</th>\n",
       "      <th>education</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>interested</th>\n",
       "      <th>party</th>\n",
       "      <th>...</th>\n",
       "      <th>torture</th>\n",
       "      <th>united_nations</th>\n",
       "      <th>war_in_afghanistan</th>\n",
       "      <th>war_on_terror</th>\n",
       "      <th>welfare</th>\n",
       "      <th>num_big_issues</th>\n",
       "      <th>start_date</th>\n",
       "      <th>age</th>\n",
       "      <th>proposition</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1289</td>\n",
       "      <td>Mangani</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Some College</td>\n",
       "      <td>Latino</td>\n",
       "      <td>Male</td>\n",
       "      <td>$75,000 to $100,000</td>\n",
       "      <td>in Women</td>\n",
       "      <td>Undecided</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Con</td>\n",
       "      <td>Con</td>\n",
       "      <td>25</td>\n",
       "      <td>2008-10-25</td>\n",
       "      <td>0.781750</td>\n",
       "      <td>Abortion Is An Inalienable Right.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1289</td>\n",
       "      <td>JBlake</td>\n",
       "      <td>0</td>\n",
       "      <td>1984-10-01</td>\n",
       "      <td>Graduate Degree</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>None</td>\n",
       "      <td>in Women</td>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>...</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Con</td>\n",
       "      <td>Con</td>\n",
       "      <td>Pro</td>\n",
       "      <td>47</td>\n",
       "      <td>2008-10-25</td>\n",
       "      <td>0.667191</td>\n",
       "      <td>Abortion Is An Inalienable Right.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1289</td>\n",
       "      <td>InquireTruth</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Graduate Degree</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>$75,000 to $100,000</td>\n",
       "      <td>in Women</td>\n",
       "      <td>Republican Party</td>\n",
       "      <td>...</td>\n",
       "      <td>Con</td>\n",
       "      <td>Con</td>\n",
       "      <td>Con</td>\n",
       "      <td>Con</td>\n",
       "      <td>Con</td>\n",
       "      <td>48</td>\n",
       "      <td>2008-10-25</td>\n",
       "      <td>0.781750</td>\n",
       "      <td>Abortion Is An Inalienable Right.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1289</td>\n",
       "      <td>KRFournier</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>$50,000 to $75,000</td>\n",
       "      <td>in Women</td>\n",
       "      <td>Republican Party</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Und</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Con</td>\n",
       "      <td>27</td>\n",
       "      <td>2008-10-25</td>\n",
       "      <td>0.781750</td>\n",
       "      <td>Abortion Is An Inalienable Right.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1824</td>\n",
       "      <td>Mangani</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Some College</td>\n",
       "      <td>Latino</td>\n",
       "      <td>Male</td>\n",
       "      <td>$75,000 to $100,000</td>\n",
       "      <td>in Women</td>\n",
       "      <td>Undecided</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Con</td>\n",
       "      <td>Con</td>\n",
       "      <td>25</td>\n",
       "      <td>2007-12-24</td>\n",
       "      <td>0.781750</td>\n",
       "      <td>Abortion Should Be Illegal.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   debate_id      voter_id  agreed_before   birthday         education  \\\n",
       "0       1289       Mangani             -1        NaT      Some College   \n",
       "1       1289        JBlake              0 1984-10-01   Graduate Degree   \n",
       "2       1289  InquireTruth              0        NaT   Graduate Degree   \n",
       "3       1289    KRFournier             -1        NaT  Bachelors Degree   \n",
       "4       1824       Mangani              1        NaT      Some College   \n",
       "\n",
       "  ethnicity gender               income interested             party  ...  \\\n",
       "0    Latino   Male  $75,000 to $100,000   in Women         Undecided  ...   \n",
       "1     White   Male                 None   in Women  Democratic Party  ...   \n",
       "2     White   Male  $75,000 to $100,000   in Women  Republican Party  ...   \n",
       "3     White   Male   $50,000 to $75,000   in Women  Republican Party  ...   \n",
       "4    Latino   Male  $75,000 to $100,000   in Women         Undecided  ...   \n",
       "\n",
       "  torture united_nations war_in_afghanistan  war_on_terror  welfare  \\\n",
       "0    None            Pro                Pro            Con      Con   \n",
       "1     Con            Pro                Con            Con      Pro   \n",
       "2     Con            Con                Con            Con      Con   \n",
       "3    None            Und               None           None      Con   \n",
       "4    None            Pro                Pro            Con      Con   \n",
       "\n",
       "   num_big_issues  start_date       age                        proposition  \\\n",
       "0              25  2008-10-25  0.781750  Abortion Is An Inalienable Right.   \n",
       "1              47  2008-10-25  0.667191  Abortion Is An Inalienable Right.   \n",
       "2              48  2008-10-25  0.781750  Abortion Is An Inalienable Right.   \n",
       "3              27  2008-10-25  0.781750  Abortion Is An Inalienable Right.   \n",
       "4              25  2007-12-24  0.781750        Abortion Should Be Illegal.   \n",
       "\n",
       "  stance  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4     -1  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge df with user demographics\n",
    "users_df = pd.read_json(\"../data/processed_data/users_df.json\")\n",
    "users_df = users_df.reset_index().rename(columns={\"index\": \"voter_id\"})\n",
    "\n",
    "issue_df = votes_df[votes_df.debate_id.isin(debates)][\n",
    "    [\"debate_id\", \"voter_id\", \"agreed_before\"]\n",
    "]\n",
    "issue_df = issue_df.merge(users_df, on=\"voter_id\")\n",
    "\n",
    "# merge with debates df to add start date for age calculation\n",
    "debates_df = pd.read_json(\"../data/processed_data/debates_df.json\")\n",
    "debates_df[\"start_date\"] = pd.to_datetime(debates_df[\"start_date\"])\n",
    "issue_df = issue_df.merge(\n",
    "    debates_df[[\"debate_id\", \"start_date\"]], on=\"debate_id\"\n",
    ")\n",
    "\n",
    "\n",
    "# age calculation\n",
    "issue_df[\"birthday\"] = pd.to_datetime(issue_df.birthday)\n",
    "issue_df[\"age\"] = (\n",
    "    issue_df.start_date - issue_df.birthday\n",
    ") / pd.Timedelta(\"365 days\")\n",
    "issue_df[\"age\"] = (issue_df.age.max() - issue_df.age) / (\n",
    "    issue_df.age.max() - issue_df.age.min()\n",
    ")\n",
    "issue_df[\"age\"] = issue_df.age.fillna(issue_df.age.mean())\n",
    "\n",
    "\n",
    "# merge with the stance\n",
    "stances = pd.read_json(path_to_stances)\n",
    "issue_df = issue_df.merge(stances, on=\"debate_id\")\n",
    "\n",
    "\n",
    "# Turn stances into -1,0,1\n",
    "issue_df[\"agreed_before\"] = issue_df.apply(\n",
    "    lambda x: to_stance(x, \"agreed_before\"), axis=1\n",
    ")\n",
    "issue_df[\"stance\"] = issue_df.apply(\n",
    "    lambda x: to_stance(x, \"stance\"), axis=1\n",
    ")\n",
    "issue_df[\"agreed_before\"] = (\n",
    "    issue_df.agreed_before * issue_df.stance\n",
    ")\n",
    "\n",
    "# display df\n",
    "issue_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features\n",
    "df_dummies = pd.get_dummies(\n",
    "    issue_df[[\"debate_id\", \"agreed_before\"] + demographic_features]\n",
    ")\n",
    "df_dummies_BI = pd.get_dummies(\n",
    "    issue_df[\n",
    "        [\"debate_id\", \"agreed_before\"] + demographic_features + big_issues_features\n",
    "    ]\n",
    ")\n",
    "\n",
    "features = [\n",
    "    col for col in df_dummies.columns if col not in [\"debate_id\", \"agreed_before\"]\n",
    "]\n",
    "features_BI = [\n",
    "    col for col in df_dummies_BI.columns if col not in [\"debate_id\", \"agreed_before\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models and collect results\n",
    "num_splits = 20\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "models = []\n",
    "accuracies = []\n",
    "confidence_intervals = []\n",
    "\n",
    "for clf in [LR_clf, GB_clf]:\n",
    "    scores = []\n",
    "    scores_BI = []\n",
    "    for i in range(num_splits):\n",
    "        # split on the debates\n",
    "        train, test = train_test_split(list(df.debate_id.unique()))\n",
    "        # get train and test set\n",
    "        df_train = df_dummies.copy()[df_dummies.debate_id.isin(train)]\n",
    "        df_test = df_dummies.copy()[df_dummies.debate_id.isin(test)]\n",
    "\n",
    "        df_train_BI = df_dummies_BI.copy()[df_dummies_BI.debate_id.isin(train)]\n",
    "        df_test_BI = df_dummies_BI.copy()[df_dummies_BI.debate_id.isin(test)]\n",
    "\n",
    "        # get features and outputs\n",
    "        X_train = pd.get_dummies(df_train[features])\n",
    "        X_test = pd.get_dummies(df_test[features])\n",
    "        y_train = df_train[\"agreed_before\"]\n",
    "        y_test = df_test[\"agreed_before\"]\n",
    "\n",
    "        # get features and outputs\n",
    "        X_train_BI = pd.get_dummies(df_train_BI[features_BI])\n",
    "        X_test_BI = pd.get_dummies(df_test_BI[features_BI])\n",
    "        y_train_BI = df_train_BI[\"agreed_before\"]\n",
    "        y_test_BI = df_test_BI[\"agreed_before\"]\n",
    "\n",
    "        score = clf.fit(X_train, y_train).score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "        score = clf.fit(X_train_BI, y_train_BI).score(X_test_BI, y_test_BI)\n",
    "        scores_BI.append(score)\n",
    "\n",
    "    if clf == LR_clf:\n",
    "        models.append(\"Logistic Regression\")\n",
    "        models.append(\"Logistic Regression\")\n",
    "    elif clf == GB_clf:\n",
    "        models.append(\"Gradient Boosting\")\n",
    "        models.append(\"Gradient Boosting\")\n",
    "\n",
    "    sample_mean = np.mean(scores)\n",
    "    sample_std = np.std(scores, ddof=1)  # using ddof=1 for sample standard deviation\n",
    "\n",
    "    # Step 2: Determine the t-value for a 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    degrees_of_freedom = len(scores) - 1\n",
    "    t_value = t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "    # Step 3: Calculate confidence interval\n",
    "    margin_of_error = t_value * (sample_std / np.sqrt(len(scores)))\n",
    "    confidence_interval = (\n",
    "        round((sample_mean - margin_of_error) * 100, 2),\n",
    "        round((sample_mean + margin_of_error) * 100, 2),\n",
    "    )\n",
    "\n",
    "    accuracies.append(round(sample_mean * 100, 2))\n",
    "    confidence_intervals.append(confidence_interval)\n",
    "\n",
    "    sample_mean = np.mean(scores_BI)\n",
    "    sample_std = np.std(scores_BI, ddof=1)  # using ddof=1 for sample standard deviation\n",
    "\n",
    "    # Step 2: Determine the t-value for a 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    degrees_of_freedom = len(scores_BI) - 1\n",
    "    t_value = t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "    # Step 3: Calculate confidence interval\n",
    "    margin_of_error = t_value * (sample_std / np.sqrt(len(scores_BI)))\n",
    "    confidence_interval = (\n",
    "        round((sample_mean - margin_of_error) * 100, 2),\n",
    "        round((sample_mean + margin_of_error) * 100, 2),\n",
    "    )\n",
    "\n",
    "    accuracies.append(round(sample_mean * 100, 2))\n",
    "    confidence_intervals.append(confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\begin{tabular}{llllrl}\n",
      "\\toprule\n",
      "Model & Issue & Big Issues & Reasoning & Accuracy & 95 \\% CI \\\\\n",
      "\\midrule\n",
      "Logistic Regression & Abortion & No & -- & 55.14 & (52.52, 57.76) \\\\\n",
      "Logistic Regression & Abortion & Yes & -- & 59.74 & (55.57, 63.91) \\\\\n",
      "Gradient Boosting & Abortion & No & -- & 56.14 & (54.42, 57.85) \\\\\n",
      "Gradient Boosting & Abortion & Yes & -- & 59.29 & (56.83, 61.75) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressions = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": models,\n",
    "        \"Issue\": [current_issue] * 4,\n",
    "        \"Big Issues\": [\"No\", \"Yes\", \"No\", \"Yes\"],\n",
    "        \"Reasoning\": [\"--\"] * 4,\n",
    "        \"Accuracy\": accuracies,\n",
    "        \"95 \\% CI\": confidence_intervals,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(regressions.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.81  &  (31.0, 50.0)\n"
     ]
    }
   ],
   "source": [
    "accuracy, _, _, ci = get_bootstrap(\n",
    "    crowd_df[(crowd_df.debate_id.isin(ABORTION)) & (crowd_df.question == \"q2\")],\n",
    "    \"q2\",\n",
    ")\n",
    "print(accuracy,\" & \", ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.43  &  (31.0, 51.0)\n"
     ]
    }
   ],
   "source": [
    "accuracy, _, _, ci = get_bootstrap(\n",
    "    crowd_df[(crowd_df.debate_id.isin(GAY_MARRIAGE)) & (crowd_df.question == \"q2\")],\n",
    "    \"q2\",\n",
    ")\n",
    "print(accuracy,\" & \", ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.83  &  (17.0, 35.0)\n"
     ]
    }
   ],
   "source": [
    "accuracy, _, _, ci = get_bootstrap(\n",
    "    crowd_df[(crowd_df.debate_id.isin(CAPITAL_PUNISHMENT)) & (crowd_df.question == \"q2\")],\n",
    "    \"q2\",\n",
    ")\n",
    "print(accuracy,\" & \", ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_name(row):\n",
    "    name = row.Model\n",
    "    if row[\"Big Issues\"] == \"Yes\":\n",
    "        name += \"-BI\"\n",
    "    if row[\"Reasoning\"] == \"Yes\":\n",
    "        name += \"-R\"\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_BI_R(row):\n",
    "    name = \"\"\n",
    "    if row[\"Big Issues\"] == \"Yes\":\n",
    "        if name == \"\":\n",
    "            name += \"BI\"\n",
    "        else:\n",
    "            name += \"-BI\"\n",
    "    if row[\"Reasoning\"] == \"Yes\":\n",
    "        if name == \"\":\n",
    "            name += \"R\"\n",
    "        else:\n",
    "            name += \"-R\"\n",
    "    if name == \"\":\n",
    "        return \"None\"\n",
    "    return name\n",
    "\n",
    "\n",
    "results[\"full_name\"] = results.apply(lambda x: get_full_name(x), axis=1)\n",
    "results[\"BI-R\"] = results.apply(lambda x: get_BI_R(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(sns.color_palette(\"bright\"))\n",
    "\n",
    "models = [\"GPT-3.5\", \"GPT-4\", \"Llama\", \"Mistral\"]\n",
    "baselines = results[~results.Model.isin(models)]\n",
    "\n",
    "sns.barplot(\n",
    "    data=results[results.Model.isin(models)], x=\"Model\", y=\"Accuracy\", hue=\"BI-R\"\n",
    ")\n",
    "c = 4\n",
    "for _, row in baselines.iterrows():\n",
    "    plt.axhline(\n",
    "        y=row[\"Accuracy\"], label=row[\"full_name\"], c=sns.color_palette(\"bright\")[c]\n",
    "    )\n",
    "    c += 1\n",
    "plt.legend()\n",
    "plt.ylim([0, 100])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.to_latex(index=False, float_format=\"%.2f\", position=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Stacked\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_q1 = full_df[(full_df.question == \"q1\")]\n",
    "full_q1 = full_q1[[\"debate_id\", \"gpt_response\", \"model\"]]\n",
    "full_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_q2 = full_df[(full_df.question == \"q2\")]\n",
    "full_q2 = full_q2[[\"debate_id\", \"voter_id\", \"gpt_response\", \"model\"]]\n",
    "full_q2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_q3 = full_df[full_df.question == \"q3\"]\n",
    "full_q3 = full_q3[[\"debate_id\", \"voter_id\", \"gpt_response\", \"model\"]]\n",
    "full_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.get_dummies(\n",
    "    full_q1.pivot(index=\"debate_id\", columns=\"model\", values=\"gpt_response\").dropna()\n",
    ").merge(ground_truth, on=\"debate_id\")\n",
    "df1[\"more_convincing_arguments\"] = df1[\"more_convincing_arguments\"].apply(\n",
    "    lambda x: 1 if x == \"Pro\" else x\n",
    ")\n",
    "df1[\"more_convincing_arguments\"] = df1[\"more_convincing_arguments\"].apply(\n",
    "    lambda x: 0 if x == \"Tie\" else x\n",
    ")\n",
    "df1[\"more_convincing_arguments\"] = df1[\"more_convincing_arguments\"].apply(\n",
    "    lambda x: -1 if x == \"Con\" else x\n",
    ")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(\n",
    "    full_q2.pivot(\n",
    "        index=[\"debate_id\", \"voter_id\"], columns=\"model\", values=\"gpt_response\"\n",
    "    )\n",
    ").merge(\n",
    "    full_df[[\"debate_id\", \"voter_id\", \"agreed_before\"]], on=[\"debate_id\", \"voter_id\"]\n",
    ")\n",
    "df2[\"agreed_before\"] = df2[\"agreed_before\"].apply(lambda x: 1 if x == \"Pro\" else x)\n",
    "df2[\"agreed_before\"] = df2[\"agreed_before\"].apply(lambda x: 0 if x == \"Tie\" else x)\n",
    "df2[\"agreed_before\"] = df2[\"agreed_before\"].apply(lambda x: -1 if x == \"Con\" else x)\n",
    "df2 = df2.drop_duplicates()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.get_dummies(\n",
    "    full_q3.pivot(\n",
    "        index=[\"debate_id\", \"voter_id\"], columns=\"model\", values=\"gpt_response\"\n",
    "    )\n",
    ").merge(\n",
    "    full_df[[\"debate_id\", \"voter_id\", \"agreed_after\"]], on=[\"debate_id\", \"voter_id\"]\n",
    ")\n",
    "df3[\"agreed_after\"] = df3[\"agreed_after\"].apply(lambda x: 1 if x == \"Pro\" else x)\n",
    "df3[\"agreed_after\"] = df3[\"agreed_after\"].apply(lambda x: 0 if x == \"Tie\" else x)\n",
    "df3[\"agreed_after\"] = df3[\"agreed_after\"].apply(lambda x: -1 if x == \"Con\" else x)\n",
    "df3 = df3.drop_duplicates()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "num_splits = 20\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "for clf in [LR_clf, GB_clf]:\n",
    "    scores = []\n",
    "    for i in range(num_splits):\n",
    "        # split on the debates\n",
    "        train, test = train_test_split(list(df1.debate_id.unique()))\n",
    "\n",
    "        # get train and test set\n",
    "        df1_train = df1.copy()[df1.debate_id.isin(train)]\n",
    "        df1_test = df1.copy()[df1.debate_id.isin(test)]\n",
    "\n",
    "        # get features and outputs\n",
    "        X_train = df1_train.drop(columns=[\"debate_id\", \"more_convincing_arguments\"])\n",
    "        X_test = df1_test.drop(columns=[\"debate_id\", \"more_convincing_arguments\"])\n",
    "        y_train = df1_train[\"more_convincing_arguments\"]\n",
    "        y_test = df1_test[\"more_convincing_arguments\"]\n",
    "\n",
    "        score = GB_clf.fit(X_train, y_train).score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Step 1: Calculate sample mean and sample standard deviation\n",
    "    sample_mean = np.mean(scores)\n",
    "    sample_std = np.std(scores, ddof=1)  # using ddof=1 for sample standard deviation\n",
    "\n",
    "    # Step 2: Determine the t-value for a 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    degrees_of_freedom = len(scores) - 1\n",
    "    t_value = t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "    # Step 3: Calculate confidence interval\n",
    "    margin_of_error = t_value * (sample_std / np.sqrt(len(scores)))\n",
    "    confidence_interval = (\n",
    "        round((sample_mean - margin_of_error) * 100, 2),\n",
    "        round((sample_mean + margin_of_error) * 100, 2),\n",
    "    )\n",
    "\n",
    "    print(\"Sample Mean:\", round(sample_mean * 100, 2))\n",
    "    print(\"Confidence Interval (95%):\", confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2, Q3\n",
    "\n",
    "num_splits = 20\n",
    "LR_clf = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "\n",
    "for df, output in zip([df2, df3], [\"agreed_before\", \"agreed_after\"]):\n",
    "    for clf in [LR_clf, GB_clf]:\n",
    "        scores = []\n",
    "        for i in range(num_splits):\n",
    "            # split on the debates\n",
    "            train, test = train_test_split(list(df.debate_id.unique()))\n",
    "\n",
    "            # get train and test set\n",
    "            df_train = df.copy()[df.debate_id.isin(train)]\n",
    "            df_test = df.copy()[df.debate_id.isin(test)]\n",
    "\n",
    "            # get features and outputs\n",
    "            X_train = df_train.drop(columns=[\"debate_id\", \"voter_id\", output])\n",
    "            X_test = df_test.drop(columns=[\"debate_id\", \"voter_id\", output])\n",
    "            y_train = df_train[output]\n",
    "            y_test = df_test[output]\n",
    "\n",
    "            score = GB_clf.fit(X_train, y_train).score(X_test, y_test)\n",
    "            scores.append(score)\n",
    "\n",
    "        sample_mean = np.mean(scores)\n",
    "        sample_std = np.std(\n",
    "            scores, ddof=1\n",
    "        )  # using ddof=1 for sample standard deviation\n",
    "\n",
    "        # Step 2: Determine the t-value for a 95% confidence interval\n",
    "        confidence_level = 0.95\n",
    "        degrees_of_freedom = len(scores) - 1\n",
    "        t_value = t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "        # Step 3: Calculate confidence interval\n",
    "        margin_of_error = t_value * (sample_std / np.sqrt(len(scores)))\n",
    "        confidence_interval = (\n",
    "            round((sample_mean - margin_of_error) * 100, 2),\n",
    "            round((sample_mean + margin_of_error) * 100, 2),\n",
    "        )\n",
    "\n",
    "        print(\"Sample Mean:\", round(sample_mean * 100, 2))\n",
    "        print(\"Confidence Interval (95%):\", confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
